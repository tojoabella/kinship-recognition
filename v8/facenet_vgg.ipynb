{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kinship_verification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4g8yMLqtS3W"
      },
      "source": [
        "#**Kinship Verification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWf8L2-Ru6ZE"
      },
      "source": [
        "MOUNT GOOGLE DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D44W7Bd6ALSQ",
        "outputId": "258f2e84-f90f-4ec7-a2a9-548e20ad8c57"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ribPmcZau-vR"
      },
      "source": [
        "INSTALL LIBS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS3ZhSjIAGgt"
      },
      "source": [
        "%%capture\n",
        "!pip install keras_vggface\n",
        "!pip install keras_applications"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4yFxckrAAZx"
      },
      "source": [
        "from collections import defaultdict\n",
        "from glob import glob\n",
        "from random import choice, sample\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import cv2\n",
        "from imageio import imread\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "'''\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Conv2D, Lambda, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "'''\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Conv2D, Lambda, Reshape\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras_vggface.utils import preprocess_input\n",
        "from keras_vggface.vggface import VGGFace"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXViO7APvFYW"
      },
      "source": [
        "TRAIN AND VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLuyuKKBAWMf"
      },
      "source": [
        "train_file_path = \"/gdrive/MyDrive/Kinship Recognition Starter/train_ds.csv\"\n",
        "train_folders_path = \"/gdrive/MyDrive/Kinship Recognition Starter/train/train-faces/\"\n",
        "\n",
        "# All images belonging to families F09** will be used to create the validation set while training the model\n",
        "# For final submission, you can add these to the training data as well\n",
        "val_famillies = \"F09\"\n",
        "\n",
        "all_images = glob(train_folders_path + \"*/*/*.jpg\") #all images\n",
        "train_images = [x for x in all_images if val_famillies not in x] #all images except for F09*\n",
        "val_images = [x for x in all_images if val_famillies in x] #all images that are F09*\n",
        "\n",
        "ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images] #family/member/ for all images\n",
        "\n",
        "train_person_to_images_map = defaultdict(list)\n",
        "for x in train_images:\n",
        "    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x) #add a training person to map\n",
        "\n",
        "val_person_to_images_map = defaultdict(list)\n",
        "for x in val_images:\n",
        "    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x) #add a validation person to map\n",
        "\n",
        "relationships = pd.read_csv(train_file_path)\n",
        "relationships = list(zip(relationships.p1.values, relationships.p2.values, relationships.relationship.values))\n",
        "relationships = [(x[0],x[1],x[2]) for x in relationships if x[0][:10] in ppl and x[1][:10] in ppl]\n",
        "\n",
        "train = [x for x in relationships if val_famillies not in x[0]]\n",
        "val = [x for x in relationships if val_famillies in x[0]]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no_jiO_7QCMk"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "def read_img(path):\n",
        "    img = image.load_img(path, target_size=(224, 224))\n",
        "    img = np.array(img).astype(np.float)\n",
        "    return preprocess_input(img, version=2)\n",
        "\n",
        "def prewhiten(x):\n",
        "    if x.ndim == 4:\n",
        "        axis = (1, 2, 3)\n",
        "        size = x[0].size\n",
        "    elif x.ndim == 3:\n",
        "        axis = (0, 1, 2)\n",
        "        size = x.size\n",
        "    else:\n",
        "        raise ValueError('Dimension should be 3 or 4')\n",
        "\n",
        "    mean = np.mean(x, axis=axis, keepdims=True)\n",
        "    std = np.std(x, axis=axis, keepdims=True)\n",
        "    std_adj = np.maximum(std, 1.0/np.sqrt(size))\n",
        "    y = (x - mean) / std_adj\n",
        "    return y\n",
        "\n",
        "def read_img_fc(path):\n",
        "    img = image.load_img(path, target_size=(160, 160))\n",
        "    img = np.array(img).astype(np.float)\n",
        "    return preprocess_input(img, version=2)\n",
        "    #img = cv2.imread(path)\n",
        "    #img = cv2.resize(img, (160, 160))\n",
        "    #img = np.array(img).astype(np.float)\n",
        "    #return prewhiten(img)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWXX-YHlW4xX"
      },
      "source": [
        "GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWh7uGe1EFXa"
      },
      "source": [
        "####DO THIS GENERATOR #####\n",
        "'''\n",
        "import copy\n",
        "\n",
        "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
        "    while True:\n",
        "        batch_tuples = sample(list_tuples, batch_size) #[('F0123/MID1/P01276_face0.jpg', 'F0644/MID2/P06777_face5.jpg', 0),...]\n",
        "        \n",
        "        labels = []\n",
        "        X1 = []\n",
        "        X2 = []\n",
        "        for tup in batch_tuples:\n",
        "            temp1 = tup[0].split('/')\n",
        "            person1 = temp1[0] + '/' + temp1[1] #person1: /F0123/MID1\n",
        "            temp2 = tup[1].split('/')\n",
        "            person2 = temp2[0] + '/' + temp2[1] #person2: /F0123/MID1\n",
        "            \n",
        "            person1_path = person_to_images_map[person1]\n",
        "            person2_path = person_to_images_map[person2]\n",
        "            length = len(person1_path) if len(person1_path) < len(person2_path) else len(person2_path)\n",
        "            length = min(1, length)\n",
        "\n",
        "            for i in range(length):\n",
        "                X1.append(person1_path[i])\n",
        "                X2.append(person2_path[i])\n",
        "                labels.append(tup[2])\n",
        "\n",
        "        X1 = np.array([read_img(x) for x in X1])\n",
        "        X1a = copy.deepcopy(X1)\n",
        "\n",
        "        X2 = np.array([read_img(x) for x in X2])\n",
        "        X2a = copy.deepcopy(X1)\n",
        "\n",
        "        yield [X1, X2, X1a, X2a], np.array(labels)\n",
        "'''\n",
        "\n",
        "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
        "    while True:\n",
        "        batch_tuples = sample(list_tuples, batch_size) #[('F0123/MID1/P01276_face0.jpg', 'F0644/MID2/P06777_face5.jpg', 0),...]\n",
        "        \n",
        "        labels = []\n",
        "        X1 = []\n",
        "        X2 = []\n",
        "        for tup in batch_tuples:\n",
        "            temp1 = tup[0].split('/')\n",
        "            person1 = temp1[0] + '/' + temp1[1] #person1: /F0123/MID1\n",
        "            temp2 = tup[1].split('/')\n",
        "            person2 = temp2[0] + '/' + temp2[1] #person2: /F0123/MID1\n",
        "            \n",
        "            person1_path = person_to_images_map[person1]\n",
        "            person2_path = person_to_images_map[person2]\n",
        "            length = len(person1_path) if len(person1_path) < len(person2_path) else len(person2_path)\n",
        "            length = min(1, length)\n",
        "\n",
        "            for i in range(length):\n",
        "                X1.append(person1_path[i])\n",
        "                X2.append(person2_path[i])\n",
        "                labels.append(tup[2])\n",
        "\n",
        "        X1a = np.array([read_img(x) for x in X1])\n",
        "        X1b = np.array([read_img_fc(x) for x in X1])\n",
        "\n",
        "        X2a = np.array([read_img(x) for x in X2])\n",
        "        X2b = np.array([read_img_fc(x) for x in X2])\n",
        "\n",
        "        yield [X1b, X2b, X1a, X2a], np.array(labels)\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDFBatfW_vTe",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "###FACENET FAILS\n",
        "'''\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from keras.models import load_model\n",
        "\n",
        "facenet_model = load_model('facenet_keras.h5')\n",
        "facenet_model.load_weights('facenet_keras_weights.h5')\n",
        "for layer in facenet_model.layers[:-3]:\n",
        "    layer.trainable = True\n",
        "facenet_model.summary()\n",
        "'''\n",
        "'''\n",
        "new_layer = Dense(10, activation='softmax', name='my_dense')\n",
        "\n",
        "inp = facenet_model.input\n",
        "out = new_layer(facenet_model.layers[-1].output)\n",
        "\n",
        "model2 = Model(inp, out)\n",
        "model2.summary(line_length=150)\n",
        "'''\n",
        "'''\n",
        "model_path = '/gdrive/MyDrive/facenet_keras.h5'\n",
        "model_fn = load_model(model_path)\n",
        "for layer in model_fn.layers[:-3]:\n",
        "    layer.trainable = True\n",
        "model_vgg = VGGFace(model='resnet50', include_top=False)\n",
        "for layer in model_vgg.layers[:-3]:\n",
        "    layer.trainable = True\n",
        "\n",
        "def lol():\n",
        "    input_1 = Input(shape=(IMG_SIZE_FN, IMG_SIZE_FN, 3))\n",
        "    input_2 = Input(shape=(IMG_SIZE_FN, IMG_SIZE_FN, 3))\n",
        "    input_3 = Input(shape=(IMG_SIZE_VGG, IMG_SIZE_VGG, 3))\n",
        "    input_4 = Input(shape=(IMG_SIZE_VGG, IMG_SIZE_VGG, 3))\n",
        "\n",
        "    x1 = model_fn(input_1)\n",
        "    x2 = model_fn(input_2)\n",
        "    x3 = model_vgg(input_3)\n",
        "    x4 = model_vgg(input_4)\n",
        "    \n",
        "    x1 = Reshape((1, 1 ,128))(x1)\n",
        "    x2 = Reshape((1, 1 ,128))(x2)\n",
        "    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n",
        "    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n",
        "\n",
        "    x1t = Lambda(lambda tensor  : K.square(tensor))(x1)\n",
        "    x2t = Lambda(lambda tensor  : K.square(tensor))(x2)\n",
        "    x3t = Lambda(lambda tensor  : K.square(tensor))(x3)\n",
        "    x4t = Lambda(lambda tensor  : K.square(tensor))(x4)\n",
        "    \n",
        "    merged_add_fn = Add()([x1, x2])\n",
        "    merged_add_vgg = Add()([x3, x4])\n",
        "    merged_sub1_fn = Subtract()([x1,x2])\n",
        "    merged_sub1_vgg = Subtract()([x3,x4])\n",
        "    merged_sub2_fn = Subtract()([x2,x1])\n",
        "    merged_sub2_vgg = Subtract()([x4,x3])\n",
        "    merged_mul1_fn = Multiply()([x1,x2])\n",
        "    merged_mul1_vgg = Multiply()([x3,x4])\n",
        "    merged_sq1_fn = Add()([x1t,x2t])\n",
        "    merged_sq1_vgg = Add()([x3t,x4t])\n",
        "    merged_sqrt_fn = Lambda(lambda tensor  : signed_sqrt(tensor))(merged_mul1_fn)\n",
        "    merged_sqrt_vgg = Lambda(lambda tensor  : signed_sqrt(tensor))(merged_mul1_vgg)\n",
        "\n",
        "    \n",
        "    merged_add_vgg = Conv2D(128 , [1,1] )(merged_add_vgg)\n",
        "    merged_sub1_vgg = Conv2D(128 , [1,1] )(merged_sub1_vgg)\n",
        "    merged_sub2_vgg = Conv2D(128 , [1,1] )(merged_sub2_vgg)\n",
        "    merged_mul1_vgg = Conv2D(128 , [1,1] )(merged_mul1_vgg)\n",
        "    merged_sq1_vgg = Conv2D(128 , [1,1] )(merged_sq1_vgg)\n",
        "    merged_sqrt_vgg = Conv2D(128 , [1,1] )(merged_sqrt_vgg)\n",
        "    \n",
        "    merged = Concatenate(axis=-1)([Flatten()(merged_add_vgg), (merged_add_fn), Flatten()(merged_sub1_vgg), (merged_sub1_fn),\n",
        "                                   Flatten()(merged_sub2_vgg), (merged_sub2_fn), Flatten()(merged_mul1_vgg), (merged_mul1_fn), \n",
        "                                   Flatten()(merged_sq1_vgg), (merged_sq1_fn), Flatten()(merged_sqrt_vgg), (merged_sqrt_fn)])\n",
        "    \n",
        "    merged = Dense(100, activation=\"relu\")(merged)\n",
        "    merged = Dropout(0.1)(merged)\n",
        "    merged = Dense(25, activation=\"relu\")(merged)\n",
        "    merged = Dropout(0.1)(merged)\n",
        "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
        "\n",
        "    model = Model([input_1, input_2, input_3, input_4], out)\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "    '''\n",
        "    '''\n",
        "def signed_sqrt(x):\n",
        "    return K.sign(x)*K.sqrt(K.abs(x)+1e-9)\n",
        "    '''\n",
        "\n",
        "'''\n",
        "def baseline_model():\n",
        "    #FACENET\n",
        "    facenet_model = load_model('/gdrive/MyDrive/facenet_keras.h5')\n",
        "    for layer in facenet_model.layers[:-3]:\n",
        "        layer.trainable = True\n",
        "    #input\n",
        "    fc_input_1 = Input(shape=(160, 160, 3))        \n",
        "    fc_input_2 = Input(shape=(160, 160, 3))        \n",
        "    #starting model\n",
        "    fn_x1 = facenet_model(fc_input_1)\n",
        "    fn_x2 = facenet_model(fc_input_2)\n",
        "    #reshaping image array for global max pool layer\n",
        "    fn_x1 = Reshape((1, 1 ,128))(fn_x1) \n",
        "    fn_x2 = Reshape((1, 1 ,128))(fn_x2)\n",
        "    #combining inputs\n",
        "    fn_x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn_x1), GlobalAvgPool2D()(fn_x1)])\n",
        "    fn_x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn_x2), GlobalAvgPool2D()(fn_x2)])\n",
        "    #adding potential features, concat to final layer before dense\n",
        "    fn_add = Add()([fn_x1, fn_x2])\n",
        "    fn_product = Multiply()([fn_x1,fn_x2])\n",
        "    fn_x = Concatenate(axis=-1)([fn_add, fn_product])\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB4_fhomW-hl"
      },
      "source": [
        "MODEL ARCHITECTURE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ilA8vokynSze"
      },
      "source": [
        "#@title\n",
        "\n",
        "def bbaseline_model():\n",
        "    input_1 = Input(shape=(224, 224, 3))\n",
        "    input_2 = Input(shape=(224, 224, 3))\n",
        "\n",
        "    base_model = VGGFace(model='resnet50', include_top=False)\n",
        "\n",
        "    for x in base_model.layers[:-2]:\n",
        "        x.trainable = True\n",
        "\n",
        "    x1 = base_model(input_1)\n",
        "    x2 = base_model(input_2)\n",
        "\n",
        "    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n",
        "    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n",
        "\n",
        "    x3 = Subtract()([x1, x2])\n",
        "    x3 = Multiply()([x3, x3])\n",
        "\n",
        "    x1_ = Multiply()([x1, x1])\n",
        "    x2_ = Multiply()([x2, x2])\n",
        "    x4 = Subtract()([x1_, x2_])\n",
        "    x = Concatenate(axis=-1)([x4, x3])\n",
        "    \n",
        "    x = Dense(512, activation=\"relu\")(x)\n",
        "    x = Dense(256, activation=\"relu\")(x)\n",
        "    x= Dropout(0.01)(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    out = Dense(1, activation=\"sigmoid\")(x)\n",
        "    \n",
        "    model = Model([input_1, input_2], out)\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.0001))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BBZJpieAi7Y"
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization, Add, Flatten\n",
        "from keras.models import load_model\n",
        "\n",
        "def baseline_model():\n",
        "    '''\n",
        "    ###VGG###\n",
        "    vgg_model = VGGFace(model='vgg16', include_top=False)\n",
        "    for x in vgg_model.layers[:-3]:\n",
        "        x.trainable = True\n",
        "    vgg_input_1 = Input(shape=(224, 224, 3))\n",
        "    vgg_input_2 = Input(shape=(224, 224, 3))\n",
        "    vgg_x1 = BatchNormalization()(vgg_input_1)\n",
        "    vgg_x2 = BatchNormalization()(vgg_input_2)\n",
        "    vgg_x1 = vgg_model(vgg_input_1)\n",
        "    vgg_x2 = vgg_model(vgg_input_2)\n",
        "    #flatten inputs\n",
        "    vgg_x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(vgg_x1), GlobalAvgPool2D()(vgg_x1)])\n",
        "    vgg_x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(vgg_x2), GlobalAvgPool2D()(vgg_x2)])\n",
        "    #adding layers\n",
        "    vgg_x3 = Subtract()([vgg_x1, vgg_x2]) #substract x1 and x2\n",
        "    vgg_x3 = Multiply()([vgg_x3, vgg_x3]) #then square it\n",
        "    vgg_x = Multiply()([vgg_x1, vgg_x2]) #multiply x1 and x2\n",
        "    vgg_x = Concatenate(axis=-1)([vgg_x, vgg_x3]) #concatenate (multiply x1 and x2) with (substract x1 and x2, then square)\n",
        "    '''\n",
        "    ###FACENET###\n",
        "    facenet_model = load_model('/gdrive/MyDrive/facenet_keras.h5')\n",
        "    facenet_model.load_weights('/gdrive/MyDrive/facenet_keras_weights.h5')\n",
        "    for layer in facenet_model.layers[:-3]:\n",
        "        layer.trainable = True\n",
        "    fc_input_1 = Input(shape=(160, 160, 3))\n",
        "    fc_input_2 = Input(shape=(160, 160, 3))\n",
        "    #fc_x1 = BatchNormalization()(fc_input_1)\n",
        "    #fc_x2 = BatchNormalization()(fc_input_2)\n",
        "    fn_x1 = facenet_model(fc_input_1)\n",
        "    fn_x2 = facenet_model(fc_input_2)\n",
        "    #reshaping image array for global max pool layer\n",
        "    fn_x1 = Reshape((1, 1 ,128))(fn_x1) \n",
        "    fn_x2 = Reshape((1, 1 ,128))(fn_x2)\n",
        "    #combining inputs\n",
        "    fn_x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn_x1), GlobalAvgPool2D()(fn_x1)])\n",
        "    fn_x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn_x2), GlobalAvgPool2D()(fn_x2)])\n",
        "    #adding potential features, concat to final layer before dense\n",
        "    fn_add = Add()([fn_x1, fn_x2])\n",
        "    fn_product = Multiply()([fn_x1,fn_x2])\n",
        "    fn_x = Concatenate(axis=-1)([fn_add, fn_product])\n",
        "    fn_x = Dense(50, activation=\"relu\")(fn_x)\n",
        "    \n",
        "    ###RESNET###\n",
        "    res_model = VGGFace(model='resnet50', include_top=False)\n",
        "    for x in res_model.layers[:-3]:\n",
        "        x.trainable = True\n",
        "    res_input_1 = Input(shape=(224, 224, 3)) #input tensor shape\n",
        "    res_input_2 = Input(shape=(224, 224, 3))\n",
        "    res_x1 = BatchNormalization()(res_input_1)\n",
        "    res_x2 = BatchNormalization()(res_input_2)\n",
        "    res_x1 = res_model(res_input_1) #reshaping input of model to that of image shapes\n",
        "    res_x2 = res_model(res_input_2) #requries two resnet archs\n",
        "    #flatten inputs\n",
        "    res_x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(res_x1), GlobalAvgPool2D()(res_x1)])\n",
        "    res_x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(res_x2), GlobalAvgPool2D()(res_x2)])\n",
        "    #adding potential features, concat to final layer before dense\n",
        "    res_x3 = Subtract()([res_x2, res_x2])\n",
        "    res_x3 = Multiply()([res_x3, res_x3])\n",
        "    res_x1_ = Multiply()([res_x1, res_x1])\n",
        "    res_x2_ = Multiply()([res_x2, res_x2])\n",
        "    res_x4 = Subtract()([res_x1_, res_x2_])\n",
        "    res_x = Concatenate(axis=-1)([res_x4, res_x3])\n",
        "    res_x = Dense(100, activation=\"relu\")(res_x)\n",
        "    \n",
        "\n",
        "    #MERGE RESNET AND VGG\n",
        "    merged = Concatenate(axis=-1)([fn_x, res_x])\n",
        "    merged = Dense(20, activation=\"relu\")(merged)\n",
        "    merged = Dropout(0.01)(merged)\n",
        "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
        "    \n",
        "    model = Model([fc_input_1, fc_input_2, res_input_1, res_input_2], out)\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC3TNCLWx5RC"
      },
      "source": [
        "MODEL AND CHECKPOINTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YEQ0Q6Ui6NP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2013b26-6784-47ea-98cd-7e844f0aa796"
      },
      "source": [
        "'''\n",
        "Save the best model to your drive after each training epoch so that you can come back to it. ReduceLROnPlateau reduces the learning rate when a metric has stopped improving, in this case the validation accuracy. \n",
        "'''\n",
        "file_path = \"/gdrive/MyDrive/vgg_face_tta.h5\"\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
        "reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n",
        "callbacks_list = [checkpoint, reduce_on_plateau]\n",
        "model = baseline_model()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_16), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'batch_normalization_16/gamma:0' shape=(3,) dtype=float32>\n",
            "  <tf.Variable 'batch_normalization_16/beta:0' shape=(3,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_17), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'batch_normalization_17/gamma:0' shape=(3,) dtype=float32>\n",
            "  <tf.Variable 'batch_normalization_17/beta:0' shape=(3,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_43 (InputLayer)           [(None, 160, 160, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_44 (InputLayer)           [(None, 160, 160, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_46 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_47 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "inception_resnet_v1 (Functional (None, 128)          22808144    input_43[0][0]                   \n",
            "                                                                 input_44[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "vggface_resnet50 (Functional)   (None, None, None, 2 23561152    input_46[0][0]                   \n",
            "                                                                 input_47[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape_18 (Reshape)            (None, 1, 1, 128)    0           inception_resnet_v1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "reshape_19 (Reshape)            (None, 1, 1, 128)    0           inception_resnet_v1[1][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_36 (Global (None, 2048)         0           vggface_resnet50[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_36 (Gl (None, 2048)         0           vggface_resnet50[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_37 (Global (None, 2048)         0           vggface_resnet50[1][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_37 (Gl (None, 2048)         0           vggface_resnet50[1][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_34 (Global (None, 128)          0           reshape_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_34 (Gl (None, 128)          0           reshape_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_35 (Global (None, 128)          0           reshape_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_35 (Gl (None, 128)          0           reshape_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_58 (Concatenate)    (None, 4096)         0           global_max_pooling2d_36[0][0]    \n",
            "                                                                 global_average_pooling2d_36[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "concatenate_59 (Concatenate)    (None, 4096)         0           global_max_pooling2d_37[0][0]    \n",
            "                                                                 global_average_pooling2d_37[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "concatenate_55 (Concatenate)    (None, 256)          0           global_max_pooling2d_34[0][0]    \n",
            "                                                                 global_average_pooling2d_34[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "concatenate_56 (Concatenate)    (None, 256)          0           global_max_pooling2d_35[0][0]    \n",
            "                                                                 global_average_pooling2d_35[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "multiply_34 (Multiply)          (None, 4096)         0           concatenate_58[0][0]             \n",
            "                                                                 concatenate_58[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "multiply_35 (Multiply)          (None, 4096)         0           concatenate_59[0][0]             \n",
            "                                                                 concatenate_59[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "subtract_16 (Subtract)          (None, 4096)         0           concatenate_59[0][0]             \n",
            "                                                                 concatenate_59[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_8 (TFOpLam (None, 256)          0           concatenate_55[0][0]             \n",
            "                                                                 concatenate_56[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "multiply_32 (Multiply)          (None, 256)          0           concatenate_55[0][0]             \n",
            "                                                                 concatenate_56[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "subtract_17 (Subtract)          (None, 4096)         0           multiply_34[0][0]                \n",
            "                                                                 multiply_35[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "multiply_33 (Multiply)          (None, 4096)         0           subtract_16[0][0]                \n",
            "                                                                 subtract_16[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_57 (Concatenate)    (None, 512)          0           tf.__operators__.add_8[0][0]     \n",
            "                                                                 multiply_32[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_60 (Concatenate)    (None, 8192)         0           subtract_17[0][0]                \n",
            "                                                                 multiply_33[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 50)           25650       concatenate_57[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 100)          819300      concatenate_60[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_61 (Concatenate)    (None, 150)          0           dense_13[0][0]                   \n",
            "                                                                 dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 20)           3020        concatenate_61[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 20)           0           dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 1)            21          dropout_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 47,217,287\n",
            "Trainable params: 47,135,335\n",
            "Non-trainable params: 81,952\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpfb9vXxXYFn"
      },
      "source": [
        "FIT MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDQn3ZdZAnX2"
      },
      "source": [
        "model.fit(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=False,\n",
        "                validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=25, verbose=1,\n",
        "                workers=1, callbacks=callbacks_list, steps_per_epoch=100, validation_steps=50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZSMtMvfXd6w"
      },
      "source": [
        "PREDICTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIl075HvEfAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd0ac94-8584-425d-99ed-c6c7ca165cca"
      },
      "source": [
        "# Modify paths as per your need\n",
        "test_path = \"/gdrive/MyDrive/Kinship Recognition Starter/test/\"\n",
        "\n",
        "model = baseline_model()\n",
        "model.load_weights(\"/gdrive/MyDrive/vgg_face_tta.h5\")\n",
        "\n",
        "submission = pd.read_csv('/gdrive/MyDrive/Kinship Recognition Starter/test_ds.csv')\n",
        "predictions = []\n",
        "\n",
        "count0 = 0\n",
        "count1 = 0\n",
        "for i in range(0, len(submission.p1.values), 32):\n",
        "    if i%64 == 0:\n",
        "      print(i)\n",
        "    X1 = submission.p1.values[i:i+32]\n",
        "    X1 = np.array([read_img_fc(test_path + x) for x in X1])\n",
        "\n",
        "    X1a = submission.p1.values[i:i+32]\n",
        "    X1a = np.array([read_img(test_path + x) for x in X1a])\n",
        "\n",
        "    X2 = submission.p2.values[i:i+32]\n",
        "    X2 = np.array([read_img_fc(test_path + x) for x in X2])\n",
        "\n",
        "    X2a = submission.p2.values[i:i+32]\n",
        "    X2a = np.array([read_img(test_path + x) for x in X2a])\n",
        "\n",
        "    pred = model.predict([X1, X2, X1a, X2a]).ravel().tolist()\n",
        "    #pred = model.predict([X1, X2]).ravel().tolist()\n",
        "    predictions += pred"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_18), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'batch_normalization_18/gamma:0' shape=(3,) dtype=float32>\n",
            "  <tf.Variable 'batch_normalization_18/beta:0' shape=(3,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_19), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'batch_normalization_19/gamma:0' shape=(3,) dtype=float32>\n",
            "  <tf.Variable 'batch_normalization_19/beta:0' shape=(3,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_48 (InputLayer)           [(None, 160, 160, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_49 (InputLayer)           [(None, 160, 160, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_51 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_52 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "inception_resnet_v1 (Functional (None, 128)          22808144    input_48[0][0]                   \n",
            "                                                                 input_49[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "vggface_resnet50 (Functional)   (None, None, None, 2 23561152    input_51[0][0]                   \n",
            "                                                                 input_52[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape_20 (Reshape)            (None, 1, 1, 128)    0           inception_resnet_v1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "reshape_21 (Reshape)            (None, 1, 1, 128)    0           inception_resnet_v1[1][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_40 (Global (None, 2048)         0           vggface_resnet50[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_40 (Gl (None, 2048)         0           vggface_resnet50[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_41 (Global (None, 2048)         0           vggface_resnet50[1][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_41 (Gl (None, 2048)         0           vggface_resnet50[1][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_38 (Global (None, 128)          0           reshape_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_38 (Gl (None, 128)          0           reshape_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_39 (Global (None, 128)          0           reshape_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_39 (Gl (None, 128)          0           reshape_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_65 (Concatenate)    (None, 4096)         0           global_max_pooling2d_40[0][0]    \n",
            "                                                                 global_average_pooling2d_40[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "concatenate_66 (Concatenate)    (None, 4096)         0           global_max_pooling2d_41[0][0]    \n",
            "                                                                 global_average_pooling2d_41[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "concatenate_62 (Concatenate)    (None, 256)          0           global_max_pooling2d_38[0][0]    \n",
            "                                                                 global_average_pooling2d_38[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "concatenate_63 (Concatenate)    (None, 256)          0           global_max_pooling2d_39[0][0]    \n",
            "                                                                 global_average_pooling2d_39[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "multiply_38 (Multiply)          (None, 4096)         0           concatenate_65[0][0]             \n",
            "                                                                 concatenate_65[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "multiply_39 (Multiply)          (None, 4096)         0           concatenate_66[0][0]             \n",
            "                                                                 concatenate_66[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "subtract_18 (Subtract)          (None, 4096)         0           concatenate_66[0][0]             \n",
            "                                                                 concatenate_66[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_9 (TFOpLam (None, 256)          0           concatenate_62[0][0]             \n",
            "                                                                 concatenate_63[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "multiply_36 (Multiply)          (None, 256)          0           concatenate_62[0][0]             \n",
            "                                                                 concatenate_63[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "subtract_19 (Subtract)          (None, 4096)         0           multiply_38[0][0]                \n",
            "                                                                 multiply_39[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "multiply_37 (Multiply)          (None, 4096)         0           subtract_18[0][0]                \n",
            "                                                                 subtract_18[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_64 (Concatenate)    (None, 512)          0           tf.__operators__.add_9[0][0]     \n",
            "                                                                 multiply_36[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_67 (Concatenate)    (None, 8192)         0           subtract_19[0][0]                \n",
            "                                                                 multiply_37[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 50)           25650       concatenate_64[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, 100)          819300      concatenate_67[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_68 (Concatenate)    (None, 150)          0           dense_17[0][0]                   \n",
            "                                                                 dense_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 20)           3020        concatenate_68[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 20)           0           dense_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, 1)            21          dropout_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 47,217,287\n",
            "Trainable params: 47,135,335\n",
            "Non-trainable params: 81,952\n",
            "__________________________________________________________________________________________________\n",
            "0\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47ee435a70> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee435a70>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47ee435a70> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee435a70>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47ee435950> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee435950>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47ee435950> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee435950>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47ee435830> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee435830>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47ee435830> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee435830>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47ee435320> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee435320>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47ee435320> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee435320>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47ee435560> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee435560>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47ee435560> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee435560>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47ee435ef0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee435ef0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47ee435ef0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee435ef0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47ee435cb0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee435cb0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47ee435cb0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee435cb0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47ee4358c0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee4358c0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47ee4358c0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee4358c0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47ee40f4d0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee40f4d0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47ee40f4d0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee40f4d0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47ee40f0e0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee40f0e0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47ee40f0e0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee40f0e0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47ee31ab00> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee31ab00>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47ee31ab00> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee31ab00>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47ee31acb0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee31acb0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47ee31acb0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47ee31acb0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47eabbcc20> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47eabbcc20>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47eabbcc20> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47eabbcc20>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47eabe68c0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47eabe68c0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47eabe68c0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47eabe68c0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47eabe6170> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47eabe6170>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47eabe6170> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47eabe6170>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47e91087a0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47e91087a0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47e91087a0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47e91087a0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47e9108dd0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47e9108dd0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47e9108dd0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47e9108dd0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47e91080e0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47e91080e0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47e91080e0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47e91080e0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47e9108680> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47e9108680>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47e9108680> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47e9108680>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47e911c3b0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47e911c3b0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47e911c3b0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47e911c3b0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f47e911c710> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47e911c710>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f47e911c710> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f47e911c710>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "64\n",
            "128\n",
            "192\n",
            "256\n",
            "320\n",
            "384\n",
            "448\n",
            "512\n",
            "576\n",
            "640\n",
            "704\n",
            "768\n",
            "832\n",
            "896\n",
            "960\n",
            "1024\n",
            "1088\n",
            "1152\n",
            "1216\n",
            "1280\n",
            "1344\n",
            "1408\n",
            "1472\n",
            "1536\n",
            "1600\n",
            "1664\n",
            "1728\n",
            "1792\n",
            "1856\n",
            "1920\n",
            "1984\n",
            "2048\n",
            "2112\n",
            "2176\n",
            "2240\n",
            "2304\n",
            "2368\n",
            "2432\n",
            "2496\n",
            "2560\n",
            "2624\n",
            "2688\n",
            "2752\n",
            "2816\n",
            "2880\n",
            "2944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXyzufq7iZ1g"
      },
      "source": [
        "CREATE CSV TO SUBMIT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkFEH-uva9c_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58780975-8af2-4f58-a50d-06c3f06a17fa"
      },
      "source": [
        "d = {'index': np.arange(0, 3000, 1), 'label':predictions}\n",
        "submissionfile = pd.DataFrame(data=d)\n",
        "count = 0\n",
        "count1 = 0\n",
        "count0 = 0\n",
        "for i in submissionfile.iloc[:, 1]:\n",
        "  if i < 0.04:\n",
        "    submissionfile.at[count, 'label'] = 0\n",
        "    count0 +=1\n",
        "  else:\n",
        "    submissionfile.at[count, 'label'] = 1\n",
        "    count1 +=1\n",
        "  count+=1\n",
        "  if count % 100 == 0:\n",
        "    print(count)\n",
        "print(\"1 count:\", count1)\n",
        "print(\"0 count:\", count0)\n",
        "\n",
        "submissionfile.astype(\"int64\").to_csv(\"/gdrive/MyDrive/facenet_vgg.csv\", index=False)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "1 count: 1503\n",
            "0 count: 1497\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}