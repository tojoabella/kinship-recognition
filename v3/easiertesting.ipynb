{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "easiertesting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4g8yMLqtS3W"
      },
      "source": [
        "# **Large-Scale Kinship Recognition Data Challenge: Kinship Verification STARTER NOTEBOOK**\n",
        "\n",
        "We provide framework code to get you started on the competition. The notebook is broken up into three main sections. \n",
        "1. Data Loading & Visualizing\n",
        "2. Data Generator & Model Building\n",
        "3. Training & Testing Model\n",
        "\n",
        "We have done the majority of the heavy lifting by making the data easily and readily accessible through Google Drive. Furthermore, we have made the task easier by creating a dataloader and fully trained end-to-end model that predicts a binary label (0 or 1) denoting whether two faces share a kinship relation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWf8L2-Ru6ZE"
      },
      "source": [
        "Mount to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D44W7Bd6ALSQ",
        "outputId": "932bf7bf-3f30-405b-d845-c6d143b27702"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ribPmcZau-vR"
      },
      "source": [
        "Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS3ZhSjIAGgt"
      },
      "source": [
        "%%capture\n",
        "!pip install keras_vggface\n",
        "!pip install keras_applications"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4yFxckrAAZx"
      },
      "source": [
        "from collections import defaultdict\n",
        "from glob import glob\n",
        "from random import choice, sample\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_vggface.utils import preprocess_input\n",
        "from keras_vggface.vggface import VGGFace"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXViO7APvFYW"
      },
      "source": [
        "[link text](https://)train_relationships.csv contains pairs of image paths which are positive samples (related to each other).\n",
        "\n",
        "train-faces contains the images for training itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLuyuKKBAWMf"
      },
      "source": [
        "# Modify paths as per your method of saving them\n",
        "train_file_path = \"/gdrive/MyDrive/Kinship Recognition Starter/train_ds.csv\"\n",
        "#!ls /gdrive/MyDrive/\n",
        "train_folders_path = \"/gdrive/MyDrive/Kinship Recognition Starter/train/train-faces/\"\n",
        "# All images belonging to families F09** will be used to create the validation set while training the model\n",
        "# For final submission, you can add these to the training data as well\n",
        "val_famillies = \"F09\"\n",
        "\n",
        "all_images = glob(train_folders_path + \"*/*/*.jpg\") #all images\n",
        "\n",
        "train_images = [x for x in all_images if val_famillies not in x] #all images except for F09*\n",
        "val_images = [x for x in all_images if val_famillies in x] #all images that are F09*\n",
        "\n",
        "ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images] #family/member/ for all images\n",
        "\n",
        "train_person_to_images_map = defaultdict(list)\n",
        "for x in train_images:\n",
        "    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x) #add a training person to map\n",
        "\n",
        "val_person_to_images_map = defaultdict(list)\n",
        "for x in val_images:\n",
        "    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x) #add a validation person to map\n",
        "\n",
        "relationships = pd.read_csv(train_file_path)\n",
        "relationships = list(zip(relationships.p1.values, relationships.p2.values, relationships.relationship.values))\n",
        "relationships = [(x[0],x[1],x[2]) for x in relationships if x[0][:10] in ppl and x[1][:10] in ppl]\n",
        "\n",
        "train = [x for x in relationships if val_famillies not in x[0]]\n",
        "val = [x for x in relationships if val_famillies in x[0]]\n",
        "\n",
        "from keras.preprocessing import image\n",
        "def read_img(path):\n",
        "    img = image.load_img(path, target_size=(224, 224))\n",
        "    img = np.array(img).astype(np.float)\n",
        "    return preprocess_input(img, version=2)\n",
        "\n",
        "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
        "    ppl = list(person_to_images_map.keys())\n",
        "    while True:\n",
        "        batch_tuples = sample(list_tuples, batch_size)\n",
        "        \n",
        "        # All the samples are taken from train_ds.csv, labels are in the labels column\n",
        "        labels = []\n",
        "        for tup in batch_tuples:\n",
        "          labels.append(tup[2])\n",
        "\n",
        "        X1 = [x[0] for x in batch_tuples]\n",
        "        X1 = np.array([read_img(train_folders_path + x) for x in X1])\n",
        "\n",
        "        X2 = [x[1] for x in batch_tuples]\n",
        "        X2 = np.array([read_img(train_folders_path + x) for x in X2])\n",
        "\n",
        "        yield [X1, X2], np.array(labels)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvPvyRzBw-nt"
      },
      "source": [
        "Here is an ensemble model built with two resnet-50 architectures, pre-trained, with which we can apply transfer leraning on. This model achieves the baseline and the goal is to expand on this work. There have been papers exploring different architectures as well as introducing BatchNormalization among many other techniques to improve how well the model recognizes kinship between two faces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BBZJpieAi7Y"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, Flatten, Lambda, Add\n",
        "from keras import backend as K\n",
        "\n",
        "def baseline_model():\n",
        "    input_1 = Input(shape=(224, 224, 3))\n",
        "    input_2 = Input(shape=(224, 224, 3))\n",
        "\n",
        "    base_model = VGGFace(model='resnet50', include_top=False)\n",
        "\n",
        "    for x in base_model.layers[:-3]:\n",
        "        x.trainable = True\n",
        "\n",
        "    #INPUT LAYER\n",
        "    x1 = base_model(input_1) #reshaping input of model to that of image shapes\n",
        "    x2 = base_model(input_2) #requries two resnet archs\n",
        "\n",
        "    #RESNET OUTPUT LAYER CONCAT\n",
        "    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)]) #not too sure\n",
        "    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n",
        "\n",
        "    #lambda_1 = Lambda(lambda tensor  : K.square(tensor))(x1)\n",
        "    #lambda_2 = Lambda(lambda tensor  : K.square(tensor))(x2)\n",
        "\n",
        "    #added = Add()([x1, x2])\n",
        "    #subtracted = Subtract()([x1, x2])\n",
        "    #subtracted2 = Subtract()([x2, x1])\n",
        "    #multiplied = Multiply()([x1,x2])\n",
        "    #sq_sum = Add()([lambda_1,lambda_2])\n",
        "    #sqrt = Lambda(lambda tensor  : K.sign(tensor)*K.sqrt(K.abs(tensor)+1e-9))(multiplied) #squre_root of sqrt_vgg1\n",
        "\n",
        "    x3 = Subtract()([x1, x2]) #substract x1 and x2\n",
        "    x3 = Multiply()([x3, x3]) #then square it\n",
        "\n",
        "    x = Multiply()([x1, x2]) #multiply x1 and x2\n",
        "    x = Concatenate(axis=-1)([x, x3]) #concatenate (multiply x1 and x2) with (substract x1 and x2, then square)\n",
        "\n",
        "    #concatenated = Concatenate(axis=-1)([Flatten()(added),\n",
        "    #                                     Flatten()(subtracted),\n",
        "    #                                     Flatten()(subtracted2), \n",
        "    #                                     Flatten()(multiplied),\n",
        "    #                                     Flatten()(sq_sum), \n",
        "    #                                     Flatten()(sqrt)])\n",
        "    \n",
        "    concatenated = Dense(500, activation=\"relu\")(x)\n",
        "    concatenated = Dropout(0.1)(concatenated)\n",
        "    concatenated = Dense(100, activation=\"relu\")(concatenated)\n",
        "    concatenated = Dropout(0.1)(concatenated)\n",
        "    concatenated = Dense(25, activation=\"relu\")(concatenated)\n",
        "    concatenated = Dropout(0.1)(concatenated)\n",
        "    out = Dense(1, activation=\"sigmoid\")(concatenated)\n",
        "    #x = Dense(100, activation=\"relu\")(x) #output 100-dimension\n",
        "    #x = Dropout(0.05)(x) #adding regularization\n",
        "    #out = Dense(1, activation=\"sigmoid\")(x) #output 1 (classification)\n",
        "\n",
        "    model = Model([input_1, input_2], out)\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC3TNCLWx5RC"
      },
      "source": [
        "Save the best model to your drive after each training epoch so that you can come back to it. ReduceLROnPlateau reduces the learning rate when a metric has stopped improving, in this case the validation accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YEQ0Q6Ui6NP",
        "outputId": "3f6352fc-21d2-4d9d-e851-24702e85528f"
      },
      "source": [
        "file_path = \"/gdrive/MyDrive/vgg_face.h5\"\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
        "reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n",
        "callbacks_list = [checkpoint, reduce_on_plateau]\n",
        "model = baseline_model()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_318), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv1/7x7_s2/kernel:0' shape=(7, 7, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_318), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv1/7x7_s2/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv1/7x7_s2/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_319), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_reduce/kernel:0' shape=(1, 1, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_319), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_reduce/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_1x1_reduce/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_320), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_3x3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_320), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_3x3/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_3x3/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_321), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_proj/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_322), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_increase/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_321), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_increase/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_1x1_increase/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_322), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_proj/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_1x1_proj/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_323), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_reduce/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_323), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_reduce/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_2_1x1_reduce/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_324), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_3x3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_324), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_3x3/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_2_3x3/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_325), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_increase/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_325), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_increase/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_2_1x1_increase/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_326), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_reduce/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_326), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_reduce/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_3_1x1_reduce/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_327), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_3x3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_327), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_3x3/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_3_3x3/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_328), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_increase/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_328), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_increase/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_3_1x1_increase/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_329), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_reduce/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_329), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_330), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_330), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_331), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_proj/kernel:0' shape=(1, 1, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_332), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_331), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_332), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_proj/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_1x1_proj/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_333), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_reduce/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_333), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_2_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_334), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_334), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_2_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_335), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_335), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_2_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_336), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_reduce/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_336), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_3_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_337), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_337), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_3_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_338), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_338), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_3_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_339), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_reduce/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_339), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_4_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_340), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_340), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_4_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_341), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_341), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_4_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_342), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_reduce/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_342), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_343), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_343), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_344), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_proj/kernel:0' shape=(1, 1, 512, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_345), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_344), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_345), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_proj/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_1x1_proj/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_346), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_346), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_2_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_347), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_347), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_2_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_348), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_348), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_2_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_349), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_349), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_3_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_350), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_350), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_3_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_351), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_351), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_3_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_352), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_352), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_4_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_353), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_353), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_4_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_354), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_354), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_4_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_355), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_355), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_5_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_356), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_356), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_5_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_357), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_357), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_5_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_358), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_358), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_6_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_359), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_359), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_6_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_360), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_360), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_6_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_361), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_reduce/kernel:0' shape=(1, 1, 1024, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_361), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_reduce/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_1x1_reduce/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_362), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_3x3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_362), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_3x3/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_3x3/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_363), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_proj/kernel:0' shape=(1, 1, 1024, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_364), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_increase/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_363), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_increase/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_1x1_increase/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_364), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_proj/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_1x1_proj/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_365), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_reduce/kernel:0' shape=(1, 1, 2048, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_365), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_reduce/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_2_1x1_reduce/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_366), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_3x3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_366), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_3x3/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_2_3x3/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_367), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_increase/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_367), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_increase/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_2_1x1_increase/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_368), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_reduce/kernel:0' shape=(1, 1, 2048, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_368), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_reduce/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_3_1x1_reduce/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_369), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_3x3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_369), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_3x3/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_3_3x3/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_370), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_increase/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_370), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_increase/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_3_1x1_increase/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_371), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv1/7x7_s2/kernel:0' shape=(7, 7, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_371), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv1/7x7_s2/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv1/7x7_s2/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_372), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_reduce/kernel:0' shape=(1, 1, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_372), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_reduce/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_1x1_reduce/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_373), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_3x3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_373), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_3x3/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_3x3/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_374), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_proj/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_375), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_increase/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_374), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_increase/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_1x1_increase/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_375), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_proj/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_1x1_proj/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_376), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_reduce/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_376), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_reduce/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_2_1x1_reduce/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_377), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_3x3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_377), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_3x3/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_2_3x3/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_378), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_increase/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_378), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_increase/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_2_1x1_increase/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_379), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_reduce/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_379), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_reduce/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_3_1x1_reduce/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_380), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_3x3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_380), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_3x3/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_3_3x3/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_381), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_increase/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_381), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_increase/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_3_1x1_increase/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_382), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_reduce/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_382), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_383), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_383), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_384), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_proj/kernel:0' shape=(1, 1, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_385), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_384), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_385), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_proj/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_1x1_proj/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_386), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_reduce/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_386), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_2_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_387), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_387), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_2_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_388), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_388), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_2_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_389), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_reduce/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_389), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_3_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_390), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_390), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_3_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_391), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_391), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_3_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_392), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_reduce/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_392), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_4_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_393), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_393), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_4_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_394), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_394), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_4_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_395), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_reduce/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_395), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_396), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_396), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_397), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_proj/kernel:0' shape=(1, 1, 512, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_398), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_397), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_398), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_proj/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_1x1_proj/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_399), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_399), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_2_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_400), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_400), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_2_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_401), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_401), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_2_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_402), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_402), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_3_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_403), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_403), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_3_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_404), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_404), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_3_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_405), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_405), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_4_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_406), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_406), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_4_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_407), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_407), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_4_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_408), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_408), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_5_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_409), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_409), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_5_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_410), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_410), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_5_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_411), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_411), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_6_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_412), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_412), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_6_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_413), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_413), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_6_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_414), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_reduce/kernel:0' shape=(1, 1, 1024, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_414), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_reduce/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_1x1_reduce/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_415), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_3x3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_415), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_3x3/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_3x3/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_416), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_proj/kernel:0' shape=(1, 1, 1024, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_417), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_increase/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_416), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_increase/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_1x1_increase/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_417), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_proj/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_1x1_proj/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_418), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_reduce/kernel:0' shape=(1, 1, 2048, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_418), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_reduce/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_2_1x1_reduce/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_419), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_3x3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_419), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_3x3/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_2_3x3/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_420), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_increase/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_420), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_increase/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_2_1x1_increase/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_421), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_reduce/kernel:0' shape=(1, 1, 2048, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_421), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_reduce/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_3_1x1_reduce/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_422), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_3x3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_422), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_3x3/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_3_3x3/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_423), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_increase/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_423), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_increase/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_3_1x1_increase/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_318 (TFOpLamb (None, 112, 112, 64) 0           input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_371 (TFOpLamb (None, 112, 112, 64) 0           input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 112, 112, 64 0           tf.nn.convolution_318[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 112, 112, 64 0           tf.nn.convolution_371[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_294 (TFOpLambda)     (None, 112, 112, 64) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_343 (TFOpLambda)     (None, 112, 112, 64) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_6 (TFO (None, 55, 55, 64)   0           tf.nn.relu_294[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_7 (TFO (None, 55, 55, 64)   0           tf.nn.relu_343[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_319 (TFOpLamb (None, 55, 55, 64)   0           tf.compat.v1.nn.max_pool_6[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_372 (TFOpLamb (None, 55, 55, 64)   0           tf.compat.v1.nn.max_pool_7[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_319[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_372[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_295 (TFOpLambda)     (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_344 (TFOpLambda)     (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_320 (TFOpLamb (None, 55, 55, 64)   0           tf.nn.relu_295[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_373 (TFOpLamb (None, 55, 55, 64)   0           tf.nn.relu_344[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_320[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_373[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_296 (TFOpLambda)     (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_345 (TFOpLambda)     (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_322 (TFOpLamb (None, 55, 55, 256)  0           tf.nn.relu_296[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_321 (TFOpLamb (None, 55, 55, 256)  0           tf.compat.v1.nn.max_pool_6[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_375 (TFOpLamb (None, 55, 55, 256)  0           tf.nn.relu_345[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_374 (TFOpLamb (None, 55, 55, 256)  0           tf.compat.v1.nn.max_pool_7[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_322[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_321[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_375[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_374[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_96 (TFOpLa (None, 55, 55, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_112 (TFOpL (None, 55, 55, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_297 (TFOpLambda)     (None, 55, 55, 256)  0           tf.__operators__.add_96[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_346 (TFOpLambda)     (None, 55, 55, 256)  0           tf.__operators__.add_112[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_323 (TFOpLamb (None, 55, 55, 64)   0           tf.nn.relu_297[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_376 (TFOpLamb (None, 55, 55, 64)   0           tf.nn.relu_346[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_323[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_376[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_298 (TFOpLambda)     (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_347 (TFOpLambda)     (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_324 (TFOpLamb (None, 55, 55, 64)   0           tf.nn.relu_298[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_377 (TFOpLamb (None, 55, 55, 64)   0           tf.nn.relu_347[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_324[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_377[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_299 (TFOpLambda)     (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_348 (TFOpLambda)     (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_325 (TFOpLamb (None, 55, 55, 256)  0           tf.nn.relu_299[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_378 (TFOpLamb (None, 55, 55, 256)  0           tf.nn.relu_348[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_325[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_378[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_97 (TFOpLa (None, 55, 55, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_297[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_113 (TFOpL (None, 55, 55, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_346[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_300 (TFOpLambda)     (None, 55, 55, 256)  0           tf.__operators__.add_97[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_349 (TFOpLambda)     (None, 55, 55, 256)  0           tf.__operators__.add_113[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_326 (TFOpLamb (None, 55, 55, 64)   0           tf.nn.relu_300[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_379 (TFOpLamb (None, 55, 55, 64)   0           tf.nn.relu_349[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_326[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_379[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_301 (TFOpLambda)     (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_350 (TFOpLambda)     (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_327 (TFOpLamb (None, 55, 55, 64)   0           tf.nn.relu_301[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_380 (TFOpLamb (None, 55, 55, 64)   0           tf.nn.relu_350[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_327[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_380[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_302 (TFOpLambda)     (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_351 (TFOpLambda)     (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_328 (TFOpLamb (None, 55, 55, 256)  0           tf.nn.relu_302[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_381 (TFOpLamb (None, 55, 55, 256)  0           tf.nn.relu_351[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_328[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_381[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_98 (TFOpLa (None, 55, 55, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_300[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_114 (TFOpL (None, 55, 55, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_349[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_303 (TFOpLambda)     (None, 55, 55, 256)  0           tf.__operators__.add_98[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_352 (TFOpLambda)     (None, 55, 55, 256)  0           tf.__operators__.add_114[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_329 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_303[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_382 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_352[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_329[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_382[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_304 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_353 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_330 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_304[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_383 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_353[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_330[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_383[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_305 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_354 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_332 (TFOpLamb (None, 28, 28, 512)  0           tf.nn.relu_305[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_331 (TFOpLamb (None, 28, 28, 512)  0           tf.nn.relu_303[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_385 (TFOpLamb (None, 28, 28, 512)  0           tf.nn.relu_354[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_384 (TFOpLamb (None, 28, 28, 512)  0           tf.nn.relu_352[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_332[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_331[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_385[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_384[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_99 (TFOpLa (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_115 (TFOpL (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_306 (TFOpLambda)     (None, 28, 28, 512)  0           tf.__operators__.add_99[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_355 (TFOpLambda)     (None, 28, 28, 512)  0           tf.__operators__.add_115[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_333 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_306[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_386 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_355[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_333[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_386[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_307 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_356 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_334 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_307[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_387 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_356[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_334[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_387[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_308 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_357 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_335 (TFOpLamb (None, 28, 28, 512)  0           tf.nn.relu_308[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_388 (TFOpLamb (None, 28, 28, 512)  0           tf.nn.relu_357[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_335[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_388[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_100 (TFOpL (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_306[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_116 (TFOpL (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_355[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_309 (TFOpLambda)     (None, 28, 28, 512)  0           tf.__operators__.add_100[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_358 (TFOpLambda)     (None, 28, 28, 512)  0           tf.__operators__.add_116[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_336 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_309[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_389 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_358[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_336[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_389[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_310 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_359 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_337 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_310[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_390 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_359[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_337[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_390[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_311 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_360 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_338 (TFOpLamb (None, 28, 28, 512)  0           tf.nn.relu_311[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_391 (TFOpLamb (None, 28, 28, 512)  0           tf.nn.relu_360[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_338[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_391[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_101 (TFOpL (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_309[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_117 (TFOpL (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_358[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_312 (TFOpLambda)     (None, 28, 28, 512)  0           tf.__operators__.add_101[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_361 (TFOpLambda)     (None, 28, 28, 512)  0           tf.__operators__.add_117[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_339 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_312[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_392 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_361[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_339[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_392[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_313 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_362 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_340 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_313[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_393 (TFOpLamb (None, 28, 28, 128)  0           tf.nn.relu_362[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_340[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_393[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_314 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_363 (TFOpLambda)     (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_341 (TFOpLamb (None, 28, 28, 512)  0           tf.nn.relu_314[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_394 (TFOpLamb (None, 28, 28, 512)  0           tf.nn.relu_363[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_341[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_394[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_102 (TFOpL (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_312[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_118 (TFOpL (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_361[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_315 (TFOpLambda)     (None, 28, 28, 512)  0           tf.__operators__.add_102[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_364 (TFOpLambda)     (None, 28, 28, 512)  0           tf.__operators__.add_118[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_342 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_315[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_395 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_364[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_342[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_395[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_316 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_365 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_343 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_316[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_396 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_365[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_343[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_396[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_317 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_366 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_345 (TFOpLamb (None, 14, 14, 1024) 0           tf.nn.relu_317[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_344 (TFOpLamb (None, 14, 14, 1024) 0           tf.nn.relu_315[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_398 (TFOpLamb (None, 14, 14, 1024) 0           tf.nn.relu_366[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_397 (TFOpLamb (None, 14, 14, 1024) 0           tf.nn.relu_364[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_345[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_344[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_398[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_397[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_103 (TFOpL (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_119 (TFOpL (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_318 (TFOpLambda)     (None, 14, 14, 1024) 0           tf.__operators__.add_103[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_367 (TFOpLambda)     (None, 14, 14, 1024) 0           tf.__operators__.add_119[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_346 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_318[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_399 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_367[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_346[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_399[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_319 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_368 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_347 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_319[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_400 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_368[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_347[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_400[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_320 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_369 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_348 (TFOpLamb (None, 14, 14, 1024) 0           tf.nn.relu_320[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_401 (TFOpLamb (None, 14, 14, 1024) 0           tf.nn.relu_369[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_348[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_401[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_104 (TFOpL (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_318[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_120 (TFOpL (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_367[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_321 (TFOpLambda)     (None, 14, 14, 1024) 0           tf.__operators__.add_104[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_370 (TFOpLambda)     (None, 14, 14, 1024) 0           tf.__operators__.add_120[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_349 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_321[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_402 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_370[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_349[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_402[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_322 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_371 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_350 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_322[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_403 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_371[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_350[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_403[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_323 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_372 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_351 (TFOpLamb (None, 14, 14, 1024) 0           tf.nn.relu_323[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_404 (TFOpLamb (None, 14, 14, 1024) 0           tf.nn.relu_372[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_351[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_404[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_105 (TFOpL (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_321[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_121 (TFOpL (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_370[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_324 (TFOpLambda)     (None, 14, 14, 1024) 0           tf.__operators__.add_105[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_373 (TFOpLambda)     (None, 14, 14, 1024) 0           tf.__operators__.add_121[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_352 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_324[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_405 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_373[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_352[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_405[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_325 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_374 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_353 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_325[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_406 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_374[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_353[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_406[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_326 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_375 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_354 (TFOpLamb (None, 14, 14, 1024) 0           tf.nn.relu_326[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_407 (TFOpLamb (None, 14, 14, 1024) 0           tf.nn.relu_375[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_354[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_407[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_106 (TFOpL (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_324[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_122 (TFOpL (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_373[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_327 (TFOpLambda)     (None, 14, 14, 1024) 0           tf.__operators__.add_106[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_376 (TFOpLambda)     (None, 14, 14, 1024) 0           tf.__operators__.add_122[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_355 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_327[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_408 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_376[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_355[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_408[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_328 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_377 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_356 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_328[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_409 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_377[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_356[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_409[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_329 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_378 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_357 (TFOpLamb (None, 14, 14, 1024) 0           tf.nn.relu_329[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_410 (TFOpLamb (None, 14, 14, 1024) 0           tf.nn.relu_378[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_357[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_410[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_107 (TFOpL (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_327[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_123 (TFOpL (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_376[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_330 (TFOpLambda)     (None, 14, 14, 1024) 0           tf.__operators__.add_107[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_379 (TFOpLambda)     (None, 14, 14, 1024) 0           tf.__operators__.add_123[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_358 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_330[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_411 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_379[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_358[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_411[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_331 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_380 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_359 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_331[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_412 (TFOpLamb (None, 14, 14, 256)  0           tf.nn.relu_380[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_359[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_412[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_332 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_381 (TFOpLambda)     (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_360 (TFOpLamb (None, 14, 14, 1024) 0           tf.nn.relu_332[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_413 (TFOpLamb (None, 14, 14, 1024) 0           tf.nn.relu_381[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_360[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_413[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_108 (TFOpL (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_330[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_124 (TFOpL (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_379[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_333 (TFOpLambda)     (None, 14, 14, 1024) 0           tf.__operators__.add_108[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_382 (TFOpLambda)     (None, 14, 14, 1024) 0           tf.__operators__.add_124[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_361 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_333[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_414 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_382[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_361[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_414[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_334 (TFOpLambda)     (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_383 (TFOpLambda)     (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_362 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_334[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_415 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_383[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_362[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_415[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_335 (TFOpLambda)     (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_384 (TFOpLambda)     (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_364 (TFOpLamb (None, 7, 7, 2048)   0           tf.nn.relu_335[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_363 (TFOpLamb (None, 7, 7, 2048)   0           tf.nn.relu_333[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_417 (TFOpLamb (None, 7, 7, 2048)   0           tf.nn.relu_384[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_416 (TFOpLamb (None, 7, 7, 2048)   0           tf.nn.relu_382[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_364[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_363[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_417[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_416[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_109 (TFOpL (None, 7, 7, 2048)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_125 (TFOpL (None, 7, 7, 2048)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_336 (TFOpLambda)     (None, 7, 7, 2048)   0           tf.__operators__.add_109[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_385 (TFOpLambda)     (None, 7, 7, 2048)   0           tf.__operators__.add_125[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_365 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_336[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_418 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_385[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_365[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_418[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_337 (TFOpLambda)     (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_386 (TFOpLambda)     (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_366 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_337[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_419 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_386[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_366[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_419[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_338 (TFOpLambda)     (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_387 (TFOpLambda)     (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_367 (TFOpLamb (None, 7, 7, 2048)   0           tf.nn.relu_338[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_420 (TFOpLamb (None, 7, 7, 2048)   0           tf.nn.relu_387[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_367[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_420[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_110 (TFOpL (None, 7, 7, 2048)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_336[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_126 (TFOpL (None, 7, 7, 2048)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_385[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_339 (TFOpLambda)     (None, 7, 7, 2048)   0           tf.__operators__.add_110[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_388 (TFOpLambda)     (None, 7, 7, 2048)   0           tf.__operators__.add_126[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_368 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_339[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_421 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_388[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_368[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_421[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_340 (TFOpLambda)     (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_389 (TFOpLambda)     (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_369 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_340[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_422 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_389[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_369[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_422[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_341 (TFOpLambda)     (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_390 (TFOpLambda)     (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_370 (TFOpLamb (None, 7, 7, 2048)   0           tf.nn.relu_341[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_423 (TFOpLamb (None, 7, 7, 2048)   0           tf.nn.relu_390[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_370[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_423[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_111 (TFOpL (None, 7, 7, 2048)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_339[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_127 (TFOpL (None, 7, 7, 2048)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_388[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_342 (TFOpLambda)     (None, 7, 7, 2048)   0           tf.__operators__.add_111[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_391 (TFOpLambda)     (None, 7, 7, 2048)   0           tf.__operators__.add_127[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.avg_pool_6 (TFO (None, 1, 1, 2048)   0           tf.nn.relu_342[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.avg_pool_7 (TFO (None, 1, 1, 2048)   0           tf.nn.relu_391[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_6 (GlobalM (None, 2048)         0           tf.compat.v1.nn.avg_pool_6[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_6 (Glo (None, 2048)         0           tf.compat.v1.nn.avg_pool_6[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_7 (GlobalM (None, 2048)         0           tf.compat.v1.nn.avg_pool_7[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_7 (Glo (None, 2048)         0           tf.compat.v1.nn.avg_pool_7[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 4096)         0           global_max_pooling2d_6[0][0]     \n",
            "                                                                 global_average_pooling2d_6[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 4096)         0           global_max_pooling2d_7[0][0]     \n",
            "                                                                 global_average_pooling2d_7[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "subtract_5 (Subtract)           (None, 4096)         0           concatenate_9[0][0]              \n",
            "                                                                 concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "multiply_5 (Multiply)           (None, 4096)         0           concatenate_9[0][0]              \n",
            "                                                                 concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "multiply_4 (Multiply)           (None, 4096)         0           subtract_5[0][0]                 \n",
            "                                                                 subtract_5[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 8192)         0           multiply_5[0][0]                 \n",
            "                                                                 multiply_4[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 500)          4096500     concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 500)          0           dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 100)          50100       dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 100)          0           dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (None, 25)           2525        dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 25)           0           dense_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 1)            26          dropout_11[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 4,149,151\n",
            "Trainable params: 4,149,151\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDQn3ZdZAnX2",
        "outputId": "f387c5aa-42e5-488c-bb9e-1356c70db5e5"
      },
      "source": [
        "model.fit(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=False,\n",
        "                validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=30, verbose=1,\n",
        "                workers=1, callbacks=callbacks_list, steps_per_epoch=100, validation_steps=50)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "100/100 [==============================] - 24s 202ms/step - loss: 4.0165 - acc: 0.5788 - val_loss: 2.0549 - val_acc: 0.6275\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.62750, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 19s 192ms/step - loss: 2.3232 - acc: 0.6637 - val_loss: 1.5751 - val_acc: 0.6325\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.62750 to 0.63250, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 19s 188ms/step - loss: 1.5657 - acc: 0.7156 - val_loss: 1.2574 - val_acc: 0.6550\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.63250 to 0.65500, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 18s 185ms/step - loss: 1.2119 - acc: 0.7531 - val_loss: 1.2281 - val_acc: 0.6538\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.65500\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 19s 195ms/step - loss: 0.9315 - acc: 0.7844 - val_loss: 1.2266 - val_acc: 0.6500\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.65500\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 19s 186ms/step - loss: 0.8216 - acc: 0.8150 - val_loss: 1.2666 - val_acc: 0.6288\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.65500\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 18s 180ms/step - loss: 0.5906 - acc: 0.8456 - val_loss: 1.3659 - val_acc: 0.6037\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.65500\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 19s 186ms/step - loss: 0.5226 - acc: 0.8550 - val_loss: 1.1951 - val_acc: 0.6500\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.65500\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 19s 190ms/step - loss: 0.4824 - acc: 0.8694 - val_loss: 1.1607 - val_acc: 0.6750\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.65500 to 0.67500, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 18s 183ms/step - loss: 0.5258 - acc: 0.8719 - val_loss: 1.1462 - val_acc: 0.7013\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.67500 to 0.70125, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 18s 183ms/step - loss: 0.4378 - acc: 0.8963 - val_loss: 1.4073 - val_acc: 0.6237\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.70125\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 19s 192ms/step - loss: 0.3833 - acc: 0.8913 - val_loss: 1.4140 - val_acc: 0.6637\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.70125\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 19s 187ms/step - loss: 0.3622 - acc: 0.8956 - val_loss: 1.4286 - val_acc: 0.6775\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.70125\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 18s 186ms/step - loss: 0.3275 - acc: 0.9125 - val_loss: 1.5093 - val_acc: 0.6762\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.70125\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 19s 186ms/step - loss: 0.2468 - acc: 0.9325 - val_loss: 1.6283 - val_acc: 0.6787\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.70125\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 19s 192ms/step - loss: 0.2336 - acc: 0.9319 - val_loss: 1.7349 - val_acc: 0.6812\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.70125\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 19s 195ms/step - loss: 0.2398 - acc: 0.9388 - val_loss: 1.7130 - val_acc: 0.6513\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.70125\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 19s 187ms/step - loss: 0.2014 - acc: 0.9456 - val_loss: 1.7250 - val_acc: 0.6800\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.70125\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 19s 188ms/step - loss: 0.1901 - acc: 0.9469 - val_loss: 1.8917 - val_acc: 0.6687\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.70125\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 18s 185ms/step - loss: 0.2414 - acc: 0.9337 - val_loss: 1.9209 - val_acc: 0.6825\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.70125\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 19s 195ms/step - loss: 0.1995 - acc: 0.9431 - val_loss: 2.1182 - val_acc: 0.6925\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.70125\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 18s 182ms/step - loss: 0.1778 - acc: 0.9488 - val_loss: 2.1310 - val_acc: 0.7163\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.70125 to 0.71625, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 18s 182ms/step - loss: 0.1395 - acc: 0.9550 - val_loss: 2.0862 - val_acc: 0.7063\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.71625\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 19s 193ms/step - loss: 0.1808 - acc: 0.9538 - val_loss: 2.2791 - val_acc: 0.6750\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.71625\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 19s 189ms/step - loss: 0.1829 - acc: 0.9494 - val_loss: 2.1280 - val_acc: 0.6775\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.71625\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 18s 183ms/step - loss: 0.1435 - acc: 0.9581 - val_loss: 2.1601 - val_acc: 0.7125\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.71625\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 18s 184ms/step - loss: 0.1509 - acc: 0.9600 - val_loss: 2.2032 - val_acc: 0.7000\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.71625\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 19s 194ms/step - loss: 0.1221 - acc: 0.9650 - val_loss: 2.2068 - val_acc: 0.7150\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.71625\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 19s 186ms/step - loss: 0.1276 - acc: 0.9631 - val_loss: 2.5807 - val_acc: 0.6975\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.71625\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 18s 184ms/step - loss: 0.1278 - acc: 0.9619 - val_loss: 2.8675 - val_acc: 0.6812\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.71625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3c4e19e890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIl075HvEfAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a69c048-88ce-4225-eea0-3fb28b92482b"
      },
      "source": [
        "# Modify paths as per your need\n",
        "test_path = \"/gdrive/MyDrive/Kinship Recognition Starter/test/\"\n",
        "\n",
        "#model = baseline_model()\n",
        "#model.load_weights(\"/gdrive/MyDrive/vgg_face.h5\")\n",
        "\n",
        "submission = pd.read_csv('/gdrive/MyDrive/Kinship Recognition Starter/test_ds.csv')\n",
        "predictions = []\n",
        "\n",
        "for i in range(0, len(submission.p1.values), 32):\n",
        "    if i%100 == 0:\n",
        "      print(i)\n",
        "    X1 = submission.p1.values[i:i+32]\n",
        "    X1 = np.array([read_img(test_path + x) for x in X1])\n",
        "\n",
        "    X2 = submission.p2.values[i:i+32]\n",
        "    X2 = np.array([read_img(test_path + x) for x in X2])\n",
        "\n",
        "    pred = model.predict([X1, X2]).ravel().tolist()\n",
        "    predictions += pred"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "800\n",
            "1600\n",
            "2400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXyzufq7iZ1g"
      },
      "source": [
        "The final predictions will need to be rounded: EG 0.01 rounded to 0 and 0.78 rounded to 1. The simple .round() function is sufficient as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkFEH-uva9c_"
      },
      "source": [
        "d = {'index': np.arange(0, 3000, 1), 'label':predictions}\n",
        "submissionfile = pd.DataFrame(data=d)\n",
        "submissionfile = submissionfile.round()\n",
        "submissionfile.astype(\"int64\").to_csv(\"/gdrive/MyDrive/tojo.csv\", index=False)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNodVS9W4NMB"
      },
      "source": [
        "At this point, download the CSV and submit it on Kaggle to score your predictions.\n"
      ]
    }
  ]
}