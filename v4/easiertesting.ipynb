{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "easiertesting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4g8yMLqtS3W"
      },
      "source": [
        "# **Large-Scale Kinship Recognition Data Challenge: Kinship Verification STARTER NOTEBOOK**\n",
        "\n",
        "We provide framework code to get you started on the competition. The notebook is broken up into three main sections. \n",
        "1. Data Loading & Visualizing\n",
        "2. Data Generator & Model Building\n",
        "3. Training & Testing Model\n",
        "\n",
        "We have done the majority of the heavy lifting by making the data easily and readily accessible through Google Drive. Furthermore, we have made the task easier by creating a dataloader and fully trained end-to-end model that predicts a binary label (0 or 1) denoting whether two faces share a kinship relation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWf8L2-Ru6ZE"
      },
      "source": [
        "Mount to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D44W7Bd6ALSQ",
        "outputId": "f4dc42aa-bb17-4e9d-bfe2-f091ecd77c7f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ribPmcZau-vR"
      },
      "source": [
        "Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS3ZhSjIAGgt"
      },
      "source": [
        "%%capture\n",
        "!pip install keras_vggface\n",
        "!pip install keras_applications"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4yFxckrAAZx"
      },
      "source": [
        "from collections import defaultdict\n",
        "from glob import glob\n",
        "from random import choice, sample\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_vggface.utils import preprocess_input\n",
        "from keras_vggface.vggface import VGGFace"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXViO7APvFYW"
      },
      "source": [
        "[link text](https://)train_relationships.csv contains pairs of image paths which are positive samples (related to each other).\n",
        "\n",
        "train-faces contains the images for training itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLuyuKKBAWMf"
      },
      "source": [
        "# Modify paths as per your method of saving them\n",
        "train_file_path = \"/gdrive/MyDrive/Kinship Recognition Starter/train_ds.csv\"\n",
        "#!ls /gdrive/MyDrive/\n",
        "train_folders_path = \"/gdrive/MyDrive/Kinship Recognition Starter/train/train-faces/\"\n",
        "# All images belonging to families F09** will be used to create the validation set while training the model\n",
        "# For final submission, you can add these to the training data as well\n",
        "val_famillies = \"F09\"\n",
        "\n",
        "all_images = glob(train_folders_path + \"*/*/*.jpg\") #all images\n",
        "\n",
        "train_images = [x for x in all_images if val_famillies not in x] #all images except for F09*\n",
        "val_images = [x for x in all_images if val_famillies in x] #all images that are F09*\n",
        "\n",
        "ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images] #family/member/ for all images\n",
        "\n",
        "train_person_to_images_map = defaultdict(list)\n",
        "for x in train_images:\n",
        "    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x) #add a training person to map\n",
        "\n",
        "val_person_to_images_map = defaultdict(list)\n",
        "for x in val_images:\n",
        "    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x) #add a validation person to map\n",
        "\n",
        "relationships = pd.read_csv(train_file_path)\n",
        "relationships = list(zip(relationships.p1.values, relationships.p2.values, relationships.relationship.values))\n",
        "relationships = [(x[0],x[1],x[2]) for x in relationships if x[0][:10] in ppl and x[1][:10] in ppl]\n",
        "\n",
        "train = [x for x in relationships if val_famillies not in x[0]]\n",
        "val = [x for x in relationships if val_famillies in x[0]]\n",
        "\n",
        "from keras.preprocessing import image\n",
        "def read_img(path):\n",
        "    img = image.load_img(path, target_size=(224, 224))\n",
        "    img = np.array(img).astype(np.float)\n",
        "    return preprocess_input(img, version=2)\n",
        "\n",
        "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
        "    ppl = list(person_to_images_map.keys())\n",
        "    while True:\n",
        "        batch_tuples = sample(list_tuples, batch_size)\n",
        "        \n",
        "        # All the samples are taken from train_ds.csv, labels are in the labels column\n",
        "        labels = []\n",
        "        for tup in batch_tuples:\n",
        "          labels.append(tup[2])\n",
        "\n",
        "        X1 = [x[0] for x in batch_tuples]\n",
        "        X1 = np.array([read_img(train_folders_path + x) for x in X1])\n",
        "\n",
        "        X2 = [x[1] for x in batch_tuples]\n",
        "        X2 = np.array([read_img(train_folders_path + x) for x in X2])\n",
        "\n",
        "        yield [X1, X2], np.array(labels)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvPvyRzBw-nt"
      },
      "source": [
        "Here is an ensemble model built with two resnet-50 architectures, pre-trained, with which we can apply transfer leraning on. This model achieves the baseline and the goal is to expand on this work. There have been papers exploring different architectures as well as introducing BatchNormalization among many other techniques to improve how well the model recognizes kinship between two faces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BBZJpieAi7Y"
      },
      "source": [
        "from tensorflow.keras.layers import Flatten, Add, BatchNormalization\n",
        "#from keras_facenet import FaceNet\n",
        "#from keras.models import load_model\n",
        "#from keras import backend as K\n",
        "\n",
        "\n",
        "def baseline_model():\n",
        "\n",
        "    '''\n",
        "    #FACENET\n",
        "    #input\n",
        "    fc_input_1 = Input(shape=(160, 160, 3))        \n",
        "    fc_input_2 = Input(shape=(160, 160, 3))        \n",
        "    #starting model\n",
        "    fn_1 = facenet_model(fc_input_1)\n",
        "    fn_2 = facenet_model(fc_input_2)\n",
        "    #reshaping image array for global max pool layer\n",
        "    fn_x1 = Reshape((1, 1 ,128))(fn_1) \n",
        "    fn_x2 = Reshape((1, 1 ,128))(fn_2)\n",
        "    #combining inputs\n",
        "    fn_x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn_x1), GlobalAvgPool2D()(fn_x1)])\n",
        "    fn_x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn_x2), GlobalAvgPool2D()(fn_x2)])\n",
        "    #adding potential features, concat to final layer before dense\n",
        "    fn_add = Add()([fn_x1, fn_x2])\n",
        "    fn_product = Multiply()([fn_x1,fn_x2])\n",
        "    fn_x = Concatenate(axis=-1)([fn_add, fn_product])\n",
        "    '''\n",
        "\n",
        "    #RESNET\n",
        "    base_model = VGGFace(model='resnet50', include_top=False)\n",
        "    for x in base_model.layers[:-3]:\n",
        "        x.trainable = True\n",
        "    #input\n",
        "    vgg_input_1 = Input(shape=(224, 224, 3))\n",
        "    vgg_input_2 = Input(shape=(224, 224, 3))\n",
        "    #starting model\n",
        "    vgg_x1 = base_model(vgg_input_1) #reshaping input of model to that of image shapes\n",
        "    vgg_x2 = base_model(vgg_input_2) #requries two resnet archs\n",
        "    #combining inputs\n",
        "    vgg_x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(vgg_x1), GlobalAvgPool2D()(vgg_x1)]) #not too sure\n",
        "    vgg_x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(vgg_x2), GlobalAvgPool2D()(vgg_x2)])\n",
        "    #adding potential features, concat to final layer before dense\n",
        "    \n",
        "    '''\n",
        "    vgg_x3 = Subtract()([vgg_x1, vgg_x2]) #substract x1 and x2\n",
        "    vgg_x3 = Multiply()([vgg_x3, vgg_x3]) #then square it\n",
        "    vgg_x = Multiply()([vgg_x1, vgg_x2]) #multiply x1 and x2\n",
        "    vgg_x = Concatenate(axis=-1)([vgg_x, vgg_x3]) #concatenate (multiply x1 and x2) with (substract x1 and x2, then square)\n",
        "    '''\n",
        "    \n",
        "    vgg_x3 = Subtract()([vgg_x2, vgg_x2])\n",
        "    vgg_x3 = Multiply()([vgg_x3, vgg_x3])\n",
        "\n",
        "    vgg_x1_ = Multiply()([vgg_x1, vgg_x1])\n",
        "    vgg_x2_ = Multiply()([vgg_x2, vgg_x2])\n",
        "    vgg_x4 = Subtract()([vgg_x1_, vgg_x2_])\n",
        "    vgg_x = Concatenate(axis=-1)([vgg_x4, vgg_x3])\n",
        "    \n",
        "\n",
        "    '''\n",
        "    #lambda_1 = Lambda(lambda tensor  : K.square(tensor))(x1)\n",
        "    #lambda_2 = Lambda(lambda tensor  : K.square(tensor))(x2)\n",
        "    #added = Add()([x1, x2])\n",
        "    #subtracted = Subtract()([x1, x2])\n",
        "    #subtracted2 = Subtract()([x2, x1])\n",
        "    #multiplied = Multiply()([x1,x2])\n",
        "    #sq_sum = Add()([lambda_1,lambda_2])\n",
        "    #sqrt = Lambda(lambda tensor  : K.sign(tensor)*K.sqrt(K.abs(tensor)+1e-9))(multiplied) #squre_root of sqrt_vgg1\n",
        "    #concatenated = Concatenate(axis=-1)([Flatten()(added),\n",
        "    #                                     Flatten()(subtracted),\n",
        "    #                                     Flatten()(subtracted2), \n",
        "    #                                     Flatten()(multiplied),\n",
        "    #                                     Flatten()(sq_sum), \n",
        "    #                                     Flatten()(sqrt)])\n",
        "    '''\n",
        "\n",
        "    '''\n",
        "    #COMBINE FACENET AND RESNET\n",
        "    concatenated = Concatenate(axis=-1)([fn_x, vgg_x])\n",
        "    '''\n",
        "\n",
        "    #concatenated = Dense(500, activation=\"relu\")(vgg_x)\n",
        "    #concatenated = Dropout(0.1)(concatenated)\n",
        "    concatenated = Dense(100, activation=\"relu\")(vgg_x)\n",
        "    concatenated = Dropout(0.01)(concatenated)\n",
        "    #concatenated = Dense(25, activation=\"relu\")(concatenated)\n",
        "    #concatenated = Dropout(0.1)(concatenated)\n",
        "    out = Dense(1, activation=\"sigmoid\")(concatenated)\n",
        "    #x = Dense(100, activation=\"relu\")(x) #output 100-dimension\n",
        "    #x = Dropout(0.05)(x) #adding regularization\n",
        "    #out = Dense(1, activation=\"sigmoid\")(x) #output 1 (classification)\n",
        "\n",
        "    model = Model([vgg_input_1, vgg_input_2], out)\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
        "    '''\n",
        "    def focal_loss(gamma=2., alpha=.25):\n",
        "      def focal_loss_fixed(y_true, y_pred):\n",
        "          pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "          pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "          return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
        "      return focal_loss_fixed\n",
        "    \n",
        "    model.compile(loss=[focal_loss(alpha=.25, gamma=2)], \n",
        "                  metrics=['acc'], \n",
        "                  optimizer=Adam(0.00003))\n",
        "    '''\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC3TNCLWx5RC"
      },
      "source": [
        "Save the best model to your drive after each training epoch so that you can come back to it. ReduceLROnPlateau reduces the learning rate when a metric has stopped improving, in this case the validation accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YEQ0Q6Ui6NP",
        "outputId": "ad322140-c72d-4350-d614-3d9242f37621"
      },
      "source": [
        "file_path = \"/gdrive/MyDrive/vgg_face.h5\"\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
        "reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n",
        "callbacks_list = [checkpoint, reduce_on_plateau]\n",
        "model = baseline_model()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_resnet50.h5\n",
            "94699520/94694792 [==============================] - 6s 0us/step\n",
            "94707712/94694792 [==============================] - 6s 0us/step\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv1/7x7_s2/kernel:0' shape=(7, 7, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv1/7x7_s2/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv1/7x7_s2/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_reduce/kernel:0' shape=(1, 1, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_1), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_reduce/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_1x1_reduce/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_3x3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_2), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_3x3/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_3x3/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_proj/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_increase/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_3), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_increase/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_1x1_increase/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_4), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_proj/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_1x1_proj/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_reduce/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_5), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_reduce/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_2_1x1_reduce/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_3x3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_6), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_3x3/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_2_3x3/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_increase/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_7), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_increase/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_2_1x1_increase/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_reduce/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_8), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_reduce/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_3_1x1_reduce/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_3x3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_9), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_3x3/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_3_3x3/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_increase/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_10), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_increase/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_3_1x1_increase/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_reduce/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_11), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_12), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_proj/kernel:0' shape=(1, 1, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_14), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_13), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_14), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_proj/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_1x1_proj/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_15), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_reduce/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_15), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_2_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_16), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_16), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_2_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_17), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_17), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_2_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_18), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_reduce/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_18), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_3_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_19), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_19), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_3_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_20), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_20), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_3_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_21), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_reduce/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_21), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_4_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_22), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_22), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_4_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_23), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_23), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_4_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_24), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_reduce/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_24), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_25), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_25), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_26), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_proj/kernel:0' shape=(1, 1, 512, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_27), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_26), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_27), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_proj/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_1x1_proj/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_28), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_28), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_2_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_29), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_29), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_2_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_30), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_30), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_2_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_31), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_31), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_3_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_32), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_32), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_3_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_33), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_33), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_3_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_34), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_34), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_4_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_35), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_35), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_4_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_36), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_36), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_4_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_37), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_37), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_5_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_38), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_38), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_5_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_39), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_39), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_5_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_40), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_40), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_6_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_41), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_41), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_6_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_42), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_42), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_6_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_43), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_reduce/kernel:0' shape=(1, 1, 1024, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_43), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_reduce/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_1x1_reduce/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_44), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_3x3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_44), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_3x3/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_3x3/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_45), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_proj/kernel:0' shape=(1, 1, 1024, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_46), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_increase/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_45), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_increase/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_1x1_increase/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_46), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_proj/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_1x1_proj/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_47), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_reduce/kernel:0' shape=(1, 1, 2048, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_47), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_reduce/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_2_1x1_reduce/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_48), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_3x3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_48), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_3x3/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_2_3x3/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_49), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_increase/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_49), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_increase/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_2_1x1_increase/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_50), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_reduce/kernel:0' shape=(1, 1, 2048, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_50), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_reduce/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_3_1x1_reduce/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_51), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_3x3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_51), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_3x3/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_3_3x3/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_52), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_increase/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_52), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_increase/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_3_1x1_increase/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_53), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv1/7x7_s2/kernel:0' shape=(7, 7, 3, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_53), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv1/7x7_s2/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv1/7x7_s2/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_54), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_reduce/kernel:0' shape=(1, 1, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_54), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_reduce/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_1x1_reduce/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_55), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_3x3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_55), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_3x3/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_3x3/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_56), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_proj/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_57), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_increase/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_56), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_increase/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_1x1_increase/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_57), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_1_1x1_proj/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_1_1x1_proj/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_58), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_reduce/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_58), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_reduce/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_2_1x1_reduce/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_59), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_3x3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_59), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_3x3/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_2_3x3/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_60), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_increase/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_60), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_2_1x1_increase/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_2_1x1_increase/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_61), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_reduce/kernel:0' shape=(1, 1, 256, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_61), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_reduce/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_3_1x1_reduce/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_62), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_3x3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_62), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_3x3/bn/gamma:0' shape=(64,) dtype=float32>\n",
            "  <tf.Variable 'conv2_3_3x3/bn/beta:0' shape=(64,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_63), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_increase/kernel:0' shape=(1, 1, 64, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_63), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv2_3_1x1_increase/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv2_3_1x1_increase/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_64), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_reduce/kernel:0' shape=(1, 1, 256, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_64), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_65), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_65), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_66), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_proj/kernel:0' shape=(1, 1, 256, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_67), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_66), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_67), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_1_1x1_proj/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_1_1x1_proj/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_68), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_reduce/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_68), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_2_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_69), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_69), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_2_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_70), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_70), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_2_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_2_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_71), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_reduce/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_71), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_3_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_72), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_72), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_3_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_73), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_73), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_3_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_3_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_74), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_reduce/kernel:0' shape=(1, 1, 512, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_74), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_reduce/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_4_1x1_reduce/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_75), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_3x3/kernel:0' shape=(3, 3, 128, 128) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_75), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_3x3/bn/gamma:0' shape=(128,) dtype=float32>\n",
            "  <tf.Variable 'conv3_4_3x3/bn/beta:0' shape=(128,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_76), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_increase/kernel:0' shape=(1, 1, 128, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_76), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv3_4_1x1_increase/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv3_4_1x1_increase/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_77), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_reduce/kernel:0' shape=(1, 1, 512, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_77), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_78), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_78), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_79), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_proj/kernel:0' shape=(1, 1, 512, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_80), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_79), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_80), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_1_1x1_proj/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_1_1x1_proj/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_81), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_81), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_2_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_82), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_82), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_2_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_83), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_83), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_2_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_2_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_84), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_84), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_3_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_85), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_85), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_3_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_86), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_86), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_3_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_3_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_87), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_87), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_4_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_88), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_88), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_4_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_89), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_89), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_4_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_4_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_90), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_90), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_5_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_91), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_91), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_5_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_92), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_92), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_5_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_5_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_93), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_reduce/kernel:0' shape=(1, 1, 1024, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_93), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_reduce/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_6_1x1_reduce/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_94), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_3x3/kernel:0' shape=(3, 3, 256, 256) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_94), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_3x3/bn/gamma:0' shape=(256,) dtype=float32>\n",
            "  <tf.Variable 'conv4_6_3x3/bn/beta:0' shape=(256,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_95), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_increase/kernel:0' shape=(1, 1, 256, 1024) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_95), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv4_6_1x1_increase/bn/gamma:0' shape=(1024,) dtype=float32>\n",
            "  <tf.Variable 'conv4_6_1x1_increase/bn/beta:0' shape=(1024,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_96), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_reduce/kernel:0' shape=(1, 1, 1024, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_96), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_reduce/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_1x1_reduce/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_97), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_3x3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_97), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_3x3/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_3x3/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_98), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_proj/kernel:0' shape=(1, 1, 1024, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_99), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_increase/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_98), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_increase/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_1x1_increase/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_99), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_1_1x1_proj/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_1_1x1_proj/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_100), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_reduce/kernel:0' shape=(1, 1, 2048, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_100), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_reduce/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_2_1x1_reduce/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_101), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_3x3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_101), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_3x3/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_2_3x3/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_102), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_increase/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_102), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_2_1x1_increase/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_2_1x1_increase/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_103), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_reduce/kernel:0' shape=(1, 1, 2048, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_103), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_reduce/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_3_1x1_reduce/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_104), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_3x3/kernel:0' shape=(3, 3, 512, 512) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_104), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_3x3/bn/gamma:0' shape=(512,) dtype=float32>\n",
            "  <tf.Variable 'conv5_3_3x3/bn/beta:0' shape=(512,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.nn.convolution_105), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_increase/kernel:0' shape=(1, 1, 512, 2048) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "WARNING:tensorflow:\n",
            "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.fused_batch_norm_105), but\n",
            "are not present in its tracked objects:\n",
            "  <tf.Variable 'conv5_3_1x1_increase/bn/gamma:0' shape=(2048,) dtype=float32>\n",
            "  <tf.Variable 'conv5_3_1x1_increase/bn/beta:0' shape=(2048,) dtype=float32>\n",
            "It is possible that this is intended behavior, but it is more likely\n",
            "an omission. This is a strong indication that this layer should be\n",
            "formulated as a subclassed Layer rather than a Lambda layer.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution (TFOpLambda)  (None, 112, 112, 64) 0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_53 (TFOpLambd (None, 112, 112, 64) 0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 112, 112, 64 0           tf.nn.convolution[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 112, 112, 64 0           tf.nn.convolution_53[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu (TFOpLambda)         (None, 112, 112, 64) 0           tf.compat.v1.nn.fused_batch_norm[\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_49 (TFOpLambda)      (None, 112, 112, 64) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool (TFOpL (None, 55, 55, 64)   0           tf.nn.relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.max_pool_1 (TFO (None, 55, 55, 64)   0           tf.nn.relu_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_1 (TFOpLambda (None, 55, 55, 64)   0           tf.compat.v1.nn.max_pool[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_54 (TFOpLambd (None, 55, 55, 64)   0           tf.compat.v1.nn.max_pool_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_54[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_1 (TFOpLambda)       (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_50 (TFOpLambda)      (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_2 (TFOpLambda (None, 55, 55, 64)   0           tf.nn.relu_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_55 (TFOpLambd (None, 55, 55, 64)   0           tf.nn.relu_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_55[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_2 (TFOpLambda)       (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_51 (TFOpLambda)      (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_4 (TFOpLambda (None, 55, 55, 256)  0           tf.nn.relu_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_3 (TFOpLambda (None, 55, 55, 256)  0           tf.compat.v1.nn.max_pool[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_57 (TFOpLambd (None, 55, 55, 256)  0           tf.nn.relu_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_56 (TFOpLambd (None, 55, 55, 256)  0           tf.compat.v1.nn.max_pool_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_57[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_56[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add (TFOpLambd (None, 55, 55, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_16 (TFOpLa (None, 55, 55, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_3 (TFOpLambda)       (None, 55, 55, 256)  0           tf.__operators__.add[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_52 (TFOpLambda)      (None, 55, 55, 256)  0           tf.__operators__.add_16[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_5 (TFOpLambda (None, 55, 55, 64)   0           tf.nn.relu_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_58 (TFOpLambd (None, 55, 55, 64)   0           tf.nn.relu_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_58[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_4 (TFOpLambda)       (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_53 (TFOpLambda)      (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_6 (TFOpLambda (None, 55, 55, 64)   0           tf.nn.relu_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_59 (TFOpLambd (None, 55, 55, 64)   0           tf.nn.relu_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_59[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_5 (TFOpLambda)       (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_54 (TFOpLambda)      (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_7 (TFOpLambda (None, 55, 55, 256)  0           tf.nn.relu_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_60 (TFOpLambd (None, 55, 55, 256)  0           tf.nn.relu_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_60[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_1 (TFOpLam (None, 55, 55, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_17 (TFOpLa (None, 55, 55, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_6 (TFOpLambda)       (None, 55, 55, 256)  0           tf.__operators__.add_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_55 (TFOpLambda)      (None, 55, 55, 256)  0           tf.__operators__.add_17[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_8 (TFOpLambda (None, 55, 55, 64)   0           tf.nn.relu_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_61 (TFOpLambd (None, 55, 55, 64)   0           tf.nn.relu_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_61[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_7 (TFOpLambda)       (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_56 (TFOpLambda)      (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_9 (TFOpLambda (None, 55, 55, 64)   0           tf.nn.relu_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_62 (TFOpLambd (None, 55, 55, 64)   0           tf.nn.relu_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 64), 0           tf.nn.convolution_62[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_8 (TFOpLambda)       (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_57 (TFOpLambda)      (None, 55, 55, 64)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_10 (TFOpLambd (None, 55, 55, 256)  0           tf.nn.relu_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_63 (TFOpLambd (None, 55, 55, 256)  0           tf.nn.relu_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_10[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 55, 55, 256) 0           tf.nn.convolution_63[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_2 (TFOpLam (None, 55, 55, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_18 (TFOpLa (None, 55, 55, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_9 (TFOpLambda)       (None, 55, 55, 256)  0           tf.__operators__.add_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_58 (TFOpLambda)      (None, 55, 55, 256)  0           tf.__operators__.add_18[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_11 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_64 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_64[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_10 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_59 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_12 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_65 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_65[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_11 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_60 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_14 (TFOpLambd (None, 28, 28, 512)  0           tf.nn.relu_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_13 (TFOpLambd (None, 28, 28, 512)  0           tf.nn.relu_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_67 (TFOpLambd (None, 28, 28, 512)  0           tf.nn.relu_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_66 (TFOpLambd (None, 28, 28, 512)  0           tf.nn.relu_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_14[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_13[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_67[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_66[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_3 (TFOpLam (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_19 (TFOpLa (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_12 (TFOpLambda)      (None, 28, 28, 512)  0           tf.__operators__.add_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_61 (TFOpLambda)      (None, 28, 28, 512)  0           tf.__operators__.add_19[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_15 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_68 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_15[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_68[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_13 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_62 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_16 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_69 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_16[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_69[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_14 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_63 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_17 (TFOpLambd (None, 28, 28, 512)  0           tf.nn.relu_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_70 (TFOpLambd (None, 28, 28, 512)  0           tf.nn.relu_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_17[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_70[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_4 (TFOpLam (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_20 (TFOpLa (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_15 (TFOpLambda)      (None, 28, 28, 512)  0           tf.__operators__.add_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_64 (TFOpLambda)      (None, 28, 28, 512)  0           tf.__operators__.add_20[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_18 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_71 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_18[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_71[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_16 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_65 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_19 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_72 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_19[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_72[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_17 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_66 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_20 (TFOpLambd (None, 28, 28, 512)  0           tf.nn.relu_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_73 (TFOpLambd (None, 28, 28, 512)  0           tf.nn.relu_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_20[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_73[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_5 (TFOpLam (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_21 (TFOpLa (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_18 (TFOpLambda)      (None, 28, 28, 512)  0           tf.__operators__.add_5[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_67 (TFOpLambda)      (None, 28, 28, 512)  0           tf.__operators__.add_21[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_21 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_74 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_21[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_74[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_19 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_68 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_22 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_75 (TFOpLambd (None, 28, 28, 128)  0           tf.nn.relu_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_22[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 128) 0           tf.nn.convolution_75[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_20 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_69 (TFOpLambda)      (None, 28, 28, 128)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_23 (TFOpLambd (None, 28, 28, 512)  0           tf.nn.relu_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_76 (TFOpLambd (None, 28, 28, 512)  0           tf.nn.relu_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_23[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 28, 28, 512) 0           tf.nn.convolution_76[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_6 (TFOpLam (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_22 (TFOpLa (None, 28, 28, 512)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_21 (TFOpLambda)      (None, 28, 28, 512)  0           tf.__operators__.add_6[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_70 (TFOpLambda)      (None, 28, 28, 512)  0           tf.__operators__.add_22[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_24 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_77 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_24[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_77[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_22 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_71 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_25 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_78 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_25[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_78[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_23 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_72 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_27 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_26 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_80 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_79 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_27[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_26[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_80[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_79[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_7 (TFOpLam (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_23 (TFOpLa (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_24 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_7[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_73 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_23[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_28 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_81 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_28[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_81[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_25 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_74 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_29 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_82 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_29[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_82[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_26 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_75 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_30 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_83 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_30[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_83[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_8 (TFOpLam (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_24 (TFOpLa (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_27 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_76 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_24[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_31 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_84 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_31[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_84[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_28 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_77 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_32 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_85 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_32[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_85[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_29 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_78 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_33 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_86 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_33[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_86[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_9 (TFOpLam (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_25 (TFOpLa (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_30 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_9[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_79 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_25[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_34 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_87 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_34[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_87[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_31 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_80 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_35 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_88 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_35[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_88[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_32 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_81 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_36 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_89 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_36[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_89[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_10 (TFOpLa (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_26 (TFOpLa (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_33 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_10[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_82 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_26[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_37 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_90 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_37[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_90[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_34 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_83 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_38 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_91 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_38[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_91[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_35 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_84 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_39 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_92 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_39[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_92[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_11 (TFOpLa (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_27 (TFOpLa (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_36 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_11[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_85 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_27[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_40 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_93 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_40[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_93[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_37 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_86 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_41 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_94 (TFOpLambd (None, 14, 14, 256)  0           tf.nn.relu_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_41[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 256) 0           tf.nn.convolution_94[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_38 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_87 (TFOpLambda)      (None, 14, 14, 256)  0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_42 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_95 (TFOpLambd (None, 14, 14, 1024) 0           tf.nn.relu_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_42[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 14, 14, 1024 0           tf.nn.convolution_95[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_12 (TFOpLa (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_28 (TFOpLa (None, 14, 14, 1024) 0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_39 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_88 (TFOpLambda)      (None, 14, 14, 1024) 0           tf.__operators__.add_28[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_43 (TFOpLambd (None, 7, 7, 512)    0           tf.nn.relu_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_96 (TFOpLambd (None, 7, 7, 512)    0           tf.nn.relu_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_43[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_96[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_40 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_89 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_44 (TFOpLambd (None, 7, 7, 512)    0           tf.nn.relu_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_97 (TFOpLambd (None, 7, 7, 512)    0           tf.nn.relu_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_44[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_97[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_41 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_90 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_46 (TFOpLambd (None, 7, 7, 2048)   0           tf.nn.relu_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_45 (TFOpLambd (None, 7, 7, 2048)   0           tf.nn.relu_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_99 (TFOpLambd (None, 7, 7, 2048)   0           tf.nn.relu_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_98 (TFOpLambd (None, 7, 7, 2048)   0           tf.nn.relu_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_46[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_45[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_99[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_98[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_13 (TFOpLa (None, 7, 7, 2048)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_29 (TFOpLa (None, 7, 7, 2048)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_42 (TFOpLambda)      (None, 7, 7, 2048)   0           tf.__operators__.add_13[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_91 (TFOpLambda)      (None, 7, 7, 2048)   0           tf.__operators__.add_29[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_47 (TFOpLambd (None, 7, 7, 512)    0           tf.nn.relu_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_100 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_47[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_100[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_43 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_92 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_48 (TFOpLambd (None, 7, 7, 512)    0           tf.nn.relu_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_101 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_48[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_101[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_44 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_93 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_49 (TFOpLambd (None, 7, 7, 2048)   0           tf.nn.relu_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_102 (TFOpLamb (None, 7, 7, 2048)   0           tf.nn.relu_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_49[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_102[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_14 (TFOpLa (None, 7, 7, 2048)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_30 (TFOpLa (None, 7, 7, 2048)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_45 (TFOpLambda)      (None, 7, 7, 2048)   0           tf.__operators__.add_14[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_94 (TFOpLambda)      (None, 7, 7, 2048)   0           tf.__operators__.add_30[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_50 (TFOpLambd (None, 7, 7, 512)    0           tf.nn.relu_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_103 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_50[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_103[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_46 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_95 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_51 (TFOpLambd (None, 7, 7, 512)    0           tf.nn.relu_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_104 (TFOpLamb (None, 7, 7, 512)    0           tf.nn.relu_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_51[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 512),  0           tf.nn.convolution_104[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_47 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_96 (TFOpLambda)      (None, 7, 7, 512)    0           tf.compat.v1.nn.fused_batch_norm_\n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_52 (TFOpLambd (None, 7, 7, 2048)   0           tf.nn.relu_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.convolution_105 (TFOpLamb (None, 7, 7, 2048)   0           tf.nn.relu_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_52[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.fused_batch_nor ((None, 7, 7, 2048), 0           tf.nn.convolution_105[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_15 (TFOpLa (None, 7, 7, 2048)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_31 (TFOpLa (None, 7, 7, 2048)   0           tf.compat.v1.nn.fused_batch_norm_\n",
            "                                                                 tf.nn.relu_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_48 (TFOpLambda)      (None, 7, 7, 2048)   0           tf.__operators__.add_15[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.nn.relu_97 (TFOpLambda)      (None, 7, 7, 2048)   0           tf.__operators__.add_31[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.avg_pool (TFOpL (None, 1, 1, 2048)   0           tf.nn.relu_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.nn.avg_pool_1 (TFO (None, 1, 1, 2048)   0           tf.nn.relu_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d (GlobalMax (None, 2048)         0           tf.compat.v1.nn.avg_pool[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 2048)         0           tf.compat.v1.nn.avg_pool[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_1 (GlobalM (None, 2048)         0           tf.compat.v1.nn.avg_pool_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 2048)         0           tf.compat.v1.nn.avg_pool_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 4096)         0           global_max_pooling2d[0][0]       \n",
            "                                                                 global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 4096)         0           global_max_pooling2d_1[0][0]     \n",
            "                                                                 global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 4096)         0           concatenate[0][0]                \n",
            "                                                                 concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "multiply_2 (Multiply)           (None, 4096)         0           concatenate_1[0][0]              \n",
            "                                                                 concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "subtract (Subtract)             (None, 4096)         0           concatenate_1[0][0]              \n",
            "                                                                 concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "subtract_1 (Subtract)           (None, 4096)         0           multiply_1[0][0]                 \n",
            "                                                                 multiply_2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 4096)         0           subtract[0][0]                   \n",
            "                                                                 subtract[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 8192)         0           subtract_1[0][0]                 \n",
            "                                                                 multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 100)          819300      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 100)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            101         dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 819,401\n",
            "Trainable params: 819,401\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDQn3ZdZAnX2",
        "outputId": "91055946-90fe-4905-8698-b1a4a3fba8b9"
      },
      "source": [
        "model.fit(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=False,\n",
        "                validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=25, verbose=1,\n",
        "                workers=1, callbacks=callbacks_list, steps_per_epoch=100, validation_steps=50)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "100/100 [==============================] - 941s 9s/step - loss: 5.9932 - acc: 0.6350 - val_loss: 10.1782 - val_acc: 0.4975\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.49750, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 400s 4s/step - loss: 2.5898 - acc: 0.7731 - val_loss: 8.2800 - val_acc: 0.5500\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.49750 to 0.55000, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 220s 2s/step - loss: 1.5783 - acc: 0.8469 - val_loss: 8.4876 - val_acc: 0.5450\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.55000\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 139s 1s/step - loss: 1.3001 - acc: 0.8669 - val_loss: 9.4078 - val_acc: 0.5250\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.55000\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 116s 1s/step - loss: 0.9943 - acc: 0.8850 - val_loss: 8.5764 - val_acc: 0.5625\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.55000 to 0.56250, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 78s 784ms/step - loss: 0.9575 - acc: 0.8994 - val_loss: 9.2011 - val_acc: 0.5713\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.56250 to 0.57125, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 61s 618ms/step - loss: 0.9372 - acc: 0.8956 - val_loss: 9.5322 - val_acc: 0.5850\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.57125 to 0.58500, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 52s 529ms/step - loss: 0.6034 - acc: 0.9319 - val_loss: 10.2745 - val_acc: 0.5600\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.58500\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 48s 487ms/step - loss: 0.5695 - acc: 0.9312 - val_loss: 9.3295 - val_acc: 0.5938\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.58500 to 0.59375, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 41s 413ms/step - loss: 0.4019 - acc: 0.9463 - val_loss: 9.3347 - val_acc: 0.6112\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.59375 to 0.61125, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 35s 352ms/step - loss: 0.3790 - acc: 0.9400 - val_loss: 8.6418 - val_acc: 0.6012\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.61125\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 32s 324ms/step - loss: 0.2526 - acc: 0.9669 - val_loss: 9.5392 - val_acc: 0.6162\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.61125 to 0.61625, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 35s 353ms/step - loss: 0.3541 - acc: 0.9525 - val_loss: 8.3059 - val_acc: 0.6263\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.61625 to 0.62625, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 33s 330ms/step - loss: 0.2844 - acc: 0.9588 - val_loss: 9.5720 - val_acc: 0.6037\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.62625\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 37s 369ms/step - loss: 0.2247 - acc: 0.9663 - val_loss: 9.2586 - val_acc: 0.6237\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.62625\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 32s 326ms/step - loss: 0.2699 - acc: 0.9644 - val_loss: 9.0598 - val_acc: 0.6400\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.62625 to 0.64000, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 31s 308ms/step - loss: 0.2858 - acc: 0.9631 - val_loss: 9.4230 - val_acc: 0.6225\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.64000\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 28s 281ms/step - loss: 0.1149 - acc: 0.9775 - val_loss: 10.2986 - val_acc: 0.5987\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.64000\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 30s 301ms/step - loss: 0.2409 - acc: 0.9688 - val_loss: 9.3489 - val_acc: 0.6375\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.64000\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 27s 270ms/step - loss: 0.2273 - acc: 0.9756 - val_loss: 9.8781 - val_acc: 0.6313\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.64000\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 26s 263ms/step - loss: 0.2046 - acc: 0.9737 - val_loss: 11.4701 - val_acc: 0.6200\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.64000\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 26s 263ms/step - loss: 0.1784 - acc: 0.9794 - val_loss: 10.2476 - val_acc: 0.6350\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.64000\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 25s 253ms/step - loss: 0.1703 - acc: 0.9775 - val_loss: 10.0880 - val_acc: 0.6438\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.64000 to 0.64375, saving model to /gdrive/MyDrive/vgg_face.h5\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 24s 240ms/step - loss: 0.1035 - acc: 0.9812 - val_loss: 9.8106 - val_acc: 0.6263\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.64375\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 27s 274ms/step - loss: 0.1582 - acc: 0.9787 - val_loss: 11.2903 - val_acc: 0.6087\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.64375\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 25s 249ms/step - loss: 0.1766 - acc: 0.9800 - val_loss: 10.1762 - val_acc: 0.6250\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.64375\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 25s 254ms/step - loss: 0.0800 - acc: 0.9825 - val_loss: 9.0220 - val_acc: 0.6400\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.64375\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 26s 259ms/step - loss: 0.1157 - acc: 0.9825 - val_loss: 9.6010 - val_acc: 0.6350\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.64375\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 25s 247ms/step - loss: 0.1465 - acc: 0.9806 - val_loss: 9.9548 - val_acc: 0.6200\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.64375\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 25s 255ms/step - loss: 0.0981 - acc: 0.9869 - val_loss: 9.7254 - val_acc: 0.6438\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.64375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5f36964350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIl075HvEfAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240c7b36-cdbb-4526-f816-1517076446cb"
      },
      "source": [
        "# Modify paths as per your need\n",
        "test_path = \"/gdrive/MyDrive/Kinship Recognition Starter/test/\"\n",
        "\n",
        "#model = baseline_model()\n",
        "#model.load_weights(\"/gdrive/MyDrive/vgg_face.h5\")\n",
        "\n",
        "submission = pd.read_csv('/gdrive/MyDrive/Kinship Recognition Starter/test_ds.csv')\n",
        "predictions = []\n",
        "\n",
        "for i in range(0, len(submission.p1.values), 32):\n",
        "    if i%100 == 0:\n",
        "      print(i)\n",
        "    X1 = submission.p1.values[i:i+32]\n",
        "    X1 = np.array([read_img(test_path + x) for x in X1])\n",
        "\n",
        "    X2 = submission.p2.values[i:i+32]\n",
        "    X2 = np.array([read_img(test_path + x) for x in X2])\n",
        "\n",
        "    pred = model.predict([X1, X2]).ravel().tolist()\n",
        "    predictions += pred"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "800\n",
            "1600\n",
            "2400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXyzufq7iZ1g"
      },
      "source": [
        "The final predictions will need to be rounded: EG 0.01 rounded to 0 and 0.78 rounded to 1. The simple .round() function is sufficient as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkFEH-uva9c_"
      },
      "source": [
        "d = {'index': np.arange(0, 3000, 1), 'label':predictions}\n",
        "submissionfile = pd.DataFrame(data=d)\n",
        "submissionfile = submissionfile.round()\n",
        "submissionfile.astype(\"int64\").to_csv(\"/gdrive/MyDrive/tojo2.csv\", index=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNodVS9W4NMB"
      },
      "source": [
        "At this point, download the CSV and submit it on Kaggle to score your predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "ouGxqsORRJlF",
        "outputId": "e2f7f88e-9931-4c4d-8f0b-288688468cd4"
      },
      "source": [
        "while True: pass"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b16dc615ea65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}