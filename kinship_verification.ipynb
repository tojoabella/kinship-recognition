{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kinship-verification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4g8yMLqtS3W"
      },
      "source": [
        "#**Kinship Verification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWf8L2-Ru6ZE"
      },
      "source": [
        "MOUNT GOOGLE DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D44W7Bd6ALSQ",
        "outputId": "5b6a750b-832b-4623-a6e7-01977320889c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ribPmcZau-vR"
      },
      "source": [
        "INSTALL LIBS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS3ZhSjIAGgt"
      },
      "source": [
        "%%capture\n",
        "!pip install keras_vggface\n",
        "!pip install keras_applications"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4yFxckrAAZx"
      },
      "source": [
        "from collections import defaultdict\n",
        "from glob import glob\n",
        "from random import choice, sample\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import cv2\n",
        "from imageio import imread\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Conv2D, Lambda, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_vggface.utils import preprocess_input\n",
        "from keras_vggface.vggface import VGGFace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXViO7APvFYW"
      },
      "source": [
        "TRAIN AND VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLuyuKKBAWMf"
      },
      "source": [
        "train_file_path = \"/gdrive/MyDrive/Kinship Recognition Starter/train_ds.csv\"\n",
        "train_folders_path = \"/gdrive/MyDrive/Kinship Recognition Starter/train/train-faces/\"\n",
        "\n",
        "# All images belonging to families F09** will be used to create the validation set while training the model\n",
        "# For final submission, you can add these to the training data as well\n",
        "val_famillies = \"F09\"\n",
        "\n",
        "all_images = glob(train_folders_path + \"*/*/*.jpg\") #all images\n",
        "train_images = [x for x in all_images if val_famillies not in x] #all images except for F09*\n",
        "val_images = [x for x in all_images if val_famillies in x] #all images that are F09*\n",
        "\n",
        "ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images] #family/member/ for all images\n",
        "\n",
        "train_person_to_images_map = defaultdict(list)\n",
        "for x in train_images:\n",
        "    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x) #add a training person to map\n",
        "\n",
        "val_person_to_images_map = defaultdict(list)\n",
        "for x in val_images:\n",
        "    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x) #add a validation person to map\n",
        "\n",
        "relationships = pd.read_csv(train_file_path)\n",
        "relationships = list(zip(relationships.p1.values, relationships.p2.values, relationships.relationship.values))\n",
        "relationships = [(x[0],x[1],x[2]) for x in relationships if x[0][:10] in ppl and x[1][:10] in ppl]\n",
        "\n",
        "train = [x for x in relationships if val_famillies not in x[0]]\n",
        "val = [x for x in relationships if val_famillies in x[0]]\n",
        "\n",
        "\n",
        "from keras.preprocessing import image\n",
        "\n",
        "def read_img(path):\n",
        "    img = image.load_img(path, target_size=(224, 224))\n",
        "    img = np.array(img).astype(np.float)\n",
        "    return preprocess_input(img, version=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWXX-YHlW4xX"
      },
      "source": [
        "GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWh7uGe1EFXa"
      },
      "source": [
        "####DO THIS GENERATOR #####\n",
        "import copy\n",
        "import os\n",
        "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
        "    ppl = list(person_to_images_map.keys())\n",
        "    while True:\n",
        "        batch_tuples = sample(list_tuples, batch_size) #[('F0123/MID1/P01276_face0.jpg', 'F0644/MID2/P06777_face5.jpg', 0),...]\n",
        "        \n",
        "        labels = []\n",
        "        X1 = []\n",
        "        X2 = []\n",
        "        for tup in batch_tuples:\n",
        "            temp1 = tup[0].split('/')\n",
        "            person1 = temp1[0] + '/' + temp1[1] #person1: /F0123/MID1\n",
        "            temp2 = tup[1].split('/')\n",
        "            person2 = temp2[0] + '/' + temp2[1] #person2: /F0123/MID1\n",
        "            imgs_person1 = os.listdir(train_folders_path + person1) #imgs_person1: [P1, P2, P3...]\n",
        "            imgs_person2 = os.listdir(train_folders_path + person2)\n",
        "            length = len(imgs_person1)\n",
        "            length = len(imgs_person2) if len(imgs_person2) < length else length\n",
        "            length = min(3, length)\n",
        "\n",
        "            for i in range(length):\n",
        "                X1.append(person1 + '/' + imgs_person1[i]) #X1: [/F0123/MID1/P..., ...] 'length' times\n",
        "                X2.append(person2 + '/' + imgs_person2[i])\n",
        "                labels.append(tup[2])\n",
        "\n",
        "        #X1 = [x[0] for x in batch_tuples] #x[0] = F0123/MID1/P01276_face0.jpg\n",
        "        X1 = np.array([read_img(train_folders_path + x) for x in X1])\n",
        "        X1a = copy.deepcopy(X1)\n",
        "\n",
        "        #X2 = [x[1] for x in batch_tuples]\n",
        "        X2 = np.array([read_img(train_folders_path + x) for x in X2])\n",
        "        X2a = copy.deepcopy(X1)\n",
        "\n",
        "        yield [X1, X2, X1a, X2a], np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDFBatfW_vTe"
      },
      "source": [
        "###FACENET FAILS\n",
        "'''\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from keras.models import load_model\n",
        "\n",
        "facenet_model = load_model('facenet_keras.h5')\n",
        "facenet_model.load_weights('facenet_keras_weights.h5')\n",
        "for layer in facenet_model.layers[:-3]:\n",
        "    layer.trainable = True\n",
        "facenet_model.summary()\n",
        "'''\n",
        "'''\n",
        "new_layer = Dense(10, activation='softmax', name='my_dense')\n",
        "\n",
        "inp = facenet_model.input\n",
        "out = new_layer(facenet_model.layers[-1].output)\n",
        "\n",
        "model2 = Model(inp, out)\n",
        "model2.summary(line_length=150)\n",
        "'''\n",
        "'''\n",
        "model_path = '/gdrive/MyDrive/facenet_keras.h5'\n",
        "model_fn = load_model(model_path)\n",
        "for layer in model_fn.layers[:-3]:\n",
        "    layer.trainable = True\n",
        "model_vgg = VGGFace(model='resnet50', include_top=False)\n",
        "for layer in model_vgg.layers[:-3]:\n",
        "    layer.trainable = True\n",
        "\n",
        "def lol():\n",
        "    input_1 = Input(shape=(IMG_SIZE_FN, IMG_SIZE_FN, 3))\n",
        "    input_2 = Input(shape=(IMG_SIZE_FN, IMG_SIZE_FN, 3))\n",
        "    input_3 = Input(shape=(IMG_SIZE_VGG, IMG_SIZE_VGG, 3))\n",
        "    input_4 = Input(shape=(IMG_SIZE_VGG, IMG_SIZE_VGG, 3))\n",
        "\n",
        "    x1 = model_fn(input_1)\n",
        "    x2 = model_fn(input_2)\n",
        "    x3 = model_vgg(input_3)\n",
        "    x4 = model_vgg(input_4)\n",
        "    \n",
        "    x1 = Reshape((1, 1 ,128))(x1)\n",
        "    x2 = Reshape((1, 1 ,128))(x2)\n",
        "    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n",
        "    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n",
        "\n",
        "    x1t = Lambda(lambda tensor  : K.square(tensor))(x1)\n",
        "    x2t = Lambda(lambda tensor  : K.square(tensor))(x2)\n",
        "    x3t = Lambda(lambda tensor  : K.square(tensor))(x3)\n",
        "    x4t = Lambda(lambda tensor  : K.square(tensor))(x4)\n",
        "    \n",
        "    merged_add_fn = Add()([x1, x2])\n",
        "    merged_add_vgg = Add()([x3, x4])\n",
        "    merged_sub1_fn = Subtract()([x1,x2])\n",
        "    merged_sub1_vgg = Subtract()([x3,x4])\n",
        "    merged_sub2_fn = Subtract()([x2,x1])\n",
        "    merged_sub2_vgg = Subtract()([x4,x3])\n",
        "    merged_mul1_fn = Multiply()([x1,x2])\n",
        "    merged_mul1_vgg = Multiply()([x3,x4])\n",
        "    merged_sq1_fn = Add()([x1t,x2t])\n",
        "    merged_sq1_vgg = Add()([x3t,x4t])\n",
        "    merged_sqrt_fn = Lambda(lambda tensor  : signed_sqrt(tensor))(merged_mul1_fn)\n",
        "    merged_sqrt_vgg = Lambda(lambda tensor  : signed_sqrt(tensor))(merged_mul1_vgg)\n",
        "\n",
        "    \n",
        "    merged_add_vgg = Conv2D(128 , [1,1] )(merged_add_vgg)\n",
        "    merged_sub1_vgg = Conv2D(128 , [1,1] )(merged_sub1_vgg)\n",
        "    merged_sub2_vgg = Conv2D(128 , [1,1] )(merged_sub2_vgg)\n",
        "    merged_mul1_vgg = Conv2D(128 , [1,1] )(merged_mul1_vgg)\n",
        "    merged_sq1_vgg = Conv2D(128 , [1,1] )(merged_sq1_vgg)\n",
        "    merged_sqrt_vgg = Conv2D(128 , [1,1] )(merged_sqrt_vgg)\n",
        "    \n",
        "    merged = Concatenate(axis=-1)([Flatten()(merged_add_vgg), (merged_add_fn), Flatten()(merged_sub1_vgg), (merged_sub1_fn),\n",
        "                                   Flatten()(merged_sub2_vgg), (merged_sub2_fn), Flatten()(merged_mul1_vgg), (merged_mul1_fn), \n",
        "                                   Flatten()(merged_sq1_vgg), (merged_sq1_fn), Flatten()(merged_sqrt_vgg), (merged_sqrt_fn)])\n",
        "    \n",
        "    merged = Dense(100, activation=\"relu\")(merged)\n",
        "    merged = Dropout(0.1)(merged)\n",
        "    merged = Dense(25, activation=\"relu\")(merged)\n",
        "    merged = Dropout(0.1)(merged)\n",
        "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
        "\n",
        "    model = Model([input_1, input_2, input_3, input_4], out)\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "    '''\n",
        "    '''\n",
        "def signed_sqrt(x):\n",
        "    return K.sign(x)*K.sqrt(K.abs(x)+1e-9)\n",
        "    '''\n",
        "\n",
        "'''\n",
        "def baseline_model():\n",
        "    #FACENET\n",
        "    facenet_model = load_model('/gdrive/MyDrive/facenet_keras.h5')\n",
        "    for layer in facenet_model.layers[:-3]:\n",
        "        layer.trainable = True\n",
        "    #input\n",
        "    fc_input_1 = Input(shape=(160, 160, 3))        \n",
        "    fc_input_2 = Input(shape=(160, 160, 3))        \n",
        "    #starting model\n",
        "    fn_x1 = facenet_model(fc_input_1)\n",
        "    fn_x2 = facenet_model(fc_input_2)\n",
        "    #reshaping image array for global max pool layer\n",
        "    fn_x1 = Reshape((1, 1 ,128))(fn_x1) \n",
        "    fn_x2 = Reshape((1, 1 ,128))(fn_x2)\n",
        "    #combining inputs\n",
        "    fn_x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn_x1), GlobalAvgPool2D()(fn_x1)])\n",
        "    fn_x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn_x2), GlobalAvgPool2D()(fn_x2)])\n",
        "    #adding potential features, concat to final layer before dense\n",
        "    fn_add = Add()([fn_x1, fn_x2])\n",
        "    fn_product = Multiply()([fn_x1,fn_x2])\n",
        "    fn_x = Concatenate(axis=-1)([fn_add, fn_product])\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB4_fhomW-hl"
      },
      "source": [
        "MODEL ARCHITECTURE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BBZJpieAi7Y"
      },
      "source": [
        "from tensorflow.keras.layers import Flatten, Add, BatchNormalization\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "\n",
        "def baseline_model():\n",
        "\n",
        "    ###VGG###\n",
        "    vgg_model = VGGFace(model='vgg16', include_top=False)\n",
        "    for x in vgg_model.layers[:-3]:\n",
        "        x.trainable = True\n",
        "    vgg_input_1 = Input(shape=(224, 224, 3))\n",
        "    vgg_input_2 = Input(shape=(224, 224, 3))\n",
        "    vgg_x1 = vgg_model(vgg_input_1)\n",
        "    vgg_x2 = vgg_model(vgg_input_2)\n",
        "    #flatten inputs\n",
        "    vgg_x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(vgg_x1), GlobalAvgPool2D()(vgg_x1)])\n",
        "    vgg_x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(vgg_x2), GlobalAvgPool2D()(vgg_x2)])\n",
        "    #adding layers\n",
        "    vgg_x3 = Subtract()([vgg_x1, vgg_x2]) #substract x1 and x2\n",
        "    vgg_x3 = Multiply()([vgg_x3, vgg_x3]) #then square it\n",
        "    vgg_x = Multiply()([vgg_x1, vgg_x2]) #multiply x1 and x2\n",
        "    vgg_x = Concatenate(axis=-1)([vgg_x, vgg_x3]) #concatenate (multiply x1 and x2) with (substract x1 and x2, then square)\n",
        "\n",
        "    \n",
        "    ###RESNET###\n",
        "    res_model = VGGFace(model='resnet50', include_top=False)\n",
        "    for x in res_model.layers[:-3]:\n",
        "        x.trainable = True\n",
        "    res_input_1 = Input(shape=(224, 224, 3)) #input tensor shape\n",
        "    res_input_2 = Input(shape=(224, 224, 3))\n",
        "    res_input_1 = BatchNormalization(res_input_1)\n",
        "    res_input_2 = BatchNormalization(res_input_2)\n",
        "    res_x1 = res_model(res_input_1) #reshaping input of model to that of image shapes\n",
        "    res_x2 = res_model(res_input_2) #requries two resnet archs\n",
        "    #flatten inputs\n",
        "    res_x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(res_x1), GlobalAvgPool2D()(res_x1)])\n",
        "    res_x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(res_x2), GlobalAvgPool2D()(res_x2)])\n",
        "    #adding potential features, concat to final layer before dense\n",
        "    res_x3 = Subtract()([res_x2, res_x2])\n",
        "    res_x3 = Multiply()([res_x3, res_x3])\n",
        "    res_x1_ = Multiply()([res_x1, res_x1])\n",
        "    res_x2_ = Multiply()([res_x2, res_x2])\n",
        "    res_x4 = Subtract()([res_x1_, res_x2_])\n",
        "    res_x = Concatenate(axis=-1)([res_x4, res_x3])\n",
        "    \n",
        "\n",
        "    #MERGE RESNET AND VGG\n",
        "    merged = Concatenate(axis=-1)([vgg_x, res_x])\n",
        "    merged = Dense(100, activation=\"relu\")(merged)\n",
        "    merged = Dropout(0.01)(merged)\n",
        "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
        "    \n",
        "    model = Model([vgg_input_1, vgg_input_2, res_input_1, res_input_2], out)\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC3TNCLWx5RC"
      },
      "source": [
        "MODEL AND CHECKPOINTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YEQ0Q6Ui6NP"
      },
      "source": [
        "'''\n",
        "Save the best model to your drive after each training epoch so that you can come back to it. ReduceLROnPlateau reduces the learning rate when a metric has stopped improving, in this case the validation accuracy. \n",
        "'''\n",
        "file_path = \"/gdrive/MyDrive/vgg_face2.h5\"\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
        "reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n",
        "callbacks_list = [checkpoint, reduce_on_plateau]\n",
        "model = baseline_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpfb9vXxXYFn"
      },
      "source": [
        "FIT MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDQn3ZdZAnX2"
      },
      "source": [
        "model.fit(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=False,\n",
        "                validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=25, verbose=1,\n",
        "                workers=1, callbacks=callbacks_list, steps_per_epoch=100, validation_steps=50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZSMtMvfXd6w"
      },
      "source": [
        "PREDICTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIl075HvEfAf"
      },
      "source": [
        "# Modify paths as per your need\n",
        "test_path = \"/gdrive/MyDrive/Kinship Recognition Starter/test/\"\n",
        "\n",
        "#model = baseline_model()\n",
        "#model.load_weights(\"/gdrive/MyDrive/vgg_face.h5\")\n",
        "\n",
        "submission = pd.read_csv('/gdrive/MyDrive/Kinship Recognition Starter/test_ds.csv')\n",
        "predictions = []\n",
        "\n",
        "for i in range(0, len(submission.p1.values), 32):\n",
        "    if i%64 == 0:\n",
        "      print(i)\n",
        "    X1 = submission.p1.values[i:i+32]\n",
        "    X1 = np.array([read_img(test_path + x) for x in X1])\n",
        "\n",
        "    X1a = submission.p1.values[i:i+32]\n",
        "    X1a = np.array([read_img(test_path + x) for x in X1a])\n",
        "\n",
        "    X2 = submission.p2.values[i:i+32]\n",
        "    X2 = np.array([read_img(test_path + x) for x in X2])\n",
        "\n",
        "    X2a = submission.p2.values[i:i+32]\n",
        "    X2a = np.array([read_img(test_path + x) for x in X2a])\n",
        "\n",
        "    pred = model.predict([X1, X2, X1a, X2a]).ravel().tolist()\n",
        "    predictions += pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXyzufq7iZ1g"
      },
      "source": [
        "CREATE CSV TO SUBMIT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkFEH-uva9c_"
      },
      "source": [
        "d = {'index': np.arange(0, 3000, 1), 'label':predictions}\n",
        "submissionfile = pd.DataFrame(data=d)\n",
        "submissionfile = submissionfile.round()\n",
        "submissionfile.astype(\"int64\").to_csv(\"/gdrive/MyDrive/tojo2.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}