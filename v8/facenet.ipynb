{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kinship_verification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4g8yMLqtS3W"
      },
      "source": [
        "#**Kinship Verification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWf8L2-Ru6ZE"
      },
      "source": [
        "MOUNT GOOGLE DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D44W7Bd6ALSQ",
        "outputId": "2c632d67-40eb-4c8c-cd54-33cdebd2cb99"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ribPmcZau-vR"
      },
      "source": [
        "INSTALL LIBS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS3ZhSjIAGgt"
      },
      "source": [
        "%%capture\n",
        "!pip install keras_vggface\n",
        "!pip install keras_applications"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4yFxckrAAZx"
      },
      "source": [
        "from collections import defaultdict\n",
        "from glob import glob\n",
        "from random import choice, sample\n",
        "\n",
        "#import tensorflow as tf\n",
        "import keras\n",
        "import cv2\n",
        "from imageio import imread\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "'''\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Conv2D, Lambda, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "'''\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Conv2D, Lambda, Reshape\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras_vggface.utils import preprocess_input\n",
        "from keras_vggface.vggface import VGGFace"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXViO7APvFYW"
      },
      "source": [
        "TRAIN AND VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLuyuKKBAWMf"
      },
      "source": [
        "train_file_path = \"/gdrive/MyDrive/Kinship Recognition Starter/train_ds.csv\"\n",
        "train_folders_path = \"/gdrive/MyDrive/Kinship Recognition Starter/train/train-faces/\"\n",
        "\n",
        "# All images belonging to families F09** will be used to create the validation set while training the model\n",
        "# For final submission, you can add these to the training data as well\n",
        "val_famillies = \"F09\"\n",
        "\n",
        "all_images = glob(train_folders_path + \"*/*/*.jpg\") #all images\n",
        "train_images = [x for x in all_images if val_famillies not in x] #all images except for F09*\n",
        "val_images = [x for x in all_images if val_famillies in x] #all images that are F09*\n",
        "\n",
        "ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images] #family/member/ for all images\n",
        "\n",
        "train_person_to_images_map = defaultdict(list)\n",
        "for x in train_images:\n",
        "    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x) #add a training person to map\n",
        "\n",
        "val_person_to_images_map = defaultdict(list)\n",
        "for x in val_images:\n",
        "    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x) #add a validation person to map\n",
        "\n",
        "relationships = pd.read_csv(train_file_path)\n",
        "relationships = list(zip(relationships.p1.values, relationships.p2.values, relationships.relationship.values))\n",
        "relationships = [(x[0],x[1],x[2]) for x in relationships if x[0][:10] in ppl and x[1][:10] in ppl]\n",
        "\n",
        "train = [x for x in relationships if val_famillies not in x[0]]\n",
        "val = [x for x in relationships if val_famillies in x[0]]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNzHBFmjFmMG"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "def read_img(path):\n",
        "    img = image.load_img(path, target_size=(224, 224))\n",
        "    img = np.array(img).astype(np.float)\n",
        "    return preprocess_input(img, version=2)\n",
        "\n",
        "def prewhiten(x):\n",
        "    if x.ndim == 4:\n",
        "        axis = (1, 2, 3)\n",
        "        size = x[0].size\n",
        "    elif x.ndim == 3:\n",
        "        axis = (0, 1, 2)\n",
        "        size = x.size\n",
        "    else:\n",
        "        raise ValueError('Dimension should be 3 or 4')\n",
        "\n",
        "    mean = np.mean(x, axis=axis, keepdims=True)\n",
        "    std = np.std(x, axis=axis, keepdims=True)\n",
        "    std_adj = np.maximum(std, 1.0/np.sqrt(size))\n",
        "    y = (x - mean) / std_adj\n",
        "    return y\n",
        "\n",
        "def read_img_fc(path):\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.resize(img, (160, 160))\n",
        "    img = np.array(img).astype(np.float)\n",
        "    return prewhiten(img)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWXX-YHlW4xX"
      },
      "source": [
        "GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWh7uGe1EFXa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "b9f13f91-bfe7-41a2-e012-53e907503e6a"
      },
      "source": [
        "####DO THIS GENERATOR #####\n",
        "\n",
        "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
        "    while True:\n",
        "        batch_tuples = sample(list_tuples, batch_size) #[('F0123/MID1/P01276_face0.jpg', 'F0644/MID2/P06777_face5.jpg', 0),...]\n",
        "        \n",
        "        labels = []\n",
        "        X1 = []\n",
        "        X2 = []\n",
        "        for tup in batch_tuples:\n",
        "            temp1 = tup[0].split('/')\n",
        "            person1 = temp1[0] + '/' + temp1[1] #person1: /F0123/MID1\n",
        "            temp2 = tup[1].split('/')\n",
        "            person2 = temp2[0] + '/' + temp2[1] #person2: /F0123/MID1\n",
        "            \n",
        "            person1_path = person_to_images_map[person1]\n",
        "            person2_path = person_to_images_map[person2]\n",
        "            length = len(person1_path) if len(person1_path) < len(person2_path) else len(person2_path)\n",
        "            length = min(1, length)\n",
        "\n",
        "            for i in range(length):\n",
        "                X1.append(person1_path[i])\n",
        "                X2.append(person2_path[i])\n",
        "                labels.append(tup[2])\n",
        "\n",
        "        #X1a = np.array([read_img(x) for x in X1])\n",
        "        X1b = np.array([read_img_fc(x) for x in X1])\n",
        "\n",
        "        #X2a = np.array([read_img(x) for x in X2])\n",
        "        X2b = np.array([read_img_fc(x) for x in X2])\n",
        "\n",
        "        yield [X1b, X2b], np.array(labels)\n",
        "\n",
        "'''\n",
        "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
        "    while True:\n",
        "        batch_tuples = sample(list_tuples, batch_size) #[('F0123/MID1/P01276_face0.jpg', 'F0644/MID2/P06777_face5.jpg', 0),...]\n",
        "        \n",
        "        labels = []\n",
        "        X1 = []\n",
        "        X2 = []\n",
        "        for tup in batch_tuples:\n",
        "            temp1 = tup[0].split('/')\n",
        "            person1 = temp1[0] + '/' + temp1[1] #person1: /F0123/MID1\n",
        "            temp2 = tup[1].split('/')\n",
        "            person2 = temp2[0] + '/' + temp2[1] #person2: /F0123/MID1\n",
        "            \n",
        "            person1_path = person_to_images_map[person1]\n",
        "            person2_path = person_to_images_map[person2]\n",
        "            length = len(person1_path) if len(person1_path) < len(person2_path) else len(person2_path)\n",
        "            length = min(2, length)\n",
        "\n",
        "            for i in range(length):\n",
        "                X1.append(person1_path[i])\n",
        "                X2.append(person2_path[i])\n",
        "                labels.append(tup[2])\n",
        "\n",
        "        X1 = np.array([read_img(x) for x in X1])\n",
        "\n",
        "        X2 = np.array([read_img(x) for x in X2])\n",
        "\n",
        "        yield [X1, X2], np.array(labels)\n",
        "'''"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef gen(list_tuples, person_to_images_map, batch_size=16):\\n    while True:\\n        batch_tuples = sample(list_tuples, batch_size) #[('F0123/MID1/P01276_face0.jpg', 'F0644/MID2/P06777_face5.jpg', 0),...]\\n        \\n        labels = []\\n        X1 = []\\n        X2 = []\\n        for tup in batch_tuples:\\n            temp1 = tup[0].split('/')\\n            person1 = temp1[0] + '/' + temp1[1] #person1: /F0123/MID1\\n            temp2 = tup[1].split('/')\\n            person2 = temp2[0] + '/' + temp2[1] #person2: /F0123/MID1\\n            \\n            person1_path = person_to_images_map[person1]\\n            person2_path = person_to_images_map[person2]\\n            length = len(person1_path) if len(person1_path) < len(person2_path) else len(person2_path)\\n            length = min(2, length)\\n\\n            for i in range(length):\\n                X1.append(person1_path[i])\\n                X2.append(person2_path[i])\\n                labels.append(tup[2])\\n\\n        X1 = np.array([read_img(x) for x in X1])\\n\\n        X2 = np.array([read_img(x) for x in X2])\\n\\n        yield [X1, X2], np.array(labels)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDFBatfW_vTe",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "###FACENET FAILS\n",
        "'''\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from keras.models import load_model\n",
        "\n",
        "facenet_model = load_model('facenet_keras.h5')\n",
        "facenet_model.load_weights('facenet_keras_weights.h5')\n",
        "for layer in facenet_model.layers[:-3]:\n",
        "    layer.trainable = True\n",
        "facenet_model.summary()\n",
        "'''\n",
        "'''\n",
        "new_layer = Dense(10, activation='softmax', name='my_dense')\n",
        "\n",
        "inp = facenet_model.input\n",
        "out = new_layer(facenet_model.layers[-1].output)\n",
        "\n",
        "model2 = Model(inp, out)\n",
        "model2.summary(line_length=150)\n",
        "'''\n",
        "'''\n",
        "model_path = '/gdrive/MyDrive/facenet_keras.h5'\n",
        "model_fn = load_model(model_path)\n",
        "for layer in model_fn.layers[:-3]:\n",
        "    layer.trainable = True\n",
        "model_vgg = VGGFace(model='resnet50', include_top=False)\n",
        "for layer in model_vgg.layers[:-3]:\n",
        "    layer.trainable = True\n",
        "\n",
        "def lol():\n",
        "    input_1 = Input(shape=(IMG_SIZE_FN, IMG_SIZE_FN, 3))\n",
        "    input_2 = Input(shape=(IMG_SIZE_FN, IMG_SIZE_FN, 3))\n",
        "    input_3 = Input(shape=(IMG_SIZE_VGG, IMG_SIZE_VGG, 3))\n",
        "    input_4 = Input(shape=(IMG_SIZE_VGG, IMG_SIZE_VGG, 3))\n",
        "\n",
        "    x1 = model_fn(input_1)\n",
        "    x2 = model_fn(input_2)\n",
        "    x3 = model_vgg(input_3)\n",
        "    x4 = model_vgg(input_4)\n",
        "    \n",
        "    x1 = Reshape((1, 1 ,128))(x1)\n",
        "    x2 = Reshape((1, 1 ,128))(x2)\n",
        "    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n",
        "    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n",
        "\n",
        "    x1t = Lambda(lambda tensor  : K.square(tensor))(x1)\n",
        "    x2t = Lambda(lambda tensor  : K.square(tensor))(x2)\n",
        "    x3t = Lambda(lambda tensor  : K.square(tensor))(x3)\n",
        "    x4t = Lambda(lambda tensor  : K.square(tensor))(x4)\n",
        "    \n",
        "    merged_add_fn = Add()([x1, x2])\n",
        "    merged_add_vgg = Add()([x3, x4])\n",
        "    merged_sub1_fn = Subtract()([x1,x2])\n",
        "    merged_sub1_vgg = Subtract()([x3,x4])\n",
        "    merged_sub2_fn = Subtract()([x2,x1])\n",
        "    merged_sub2_vgg = Subtract()([x4,x3])\n",
        "    merged_mul1_fn = Multiply()([x1,x2])\n",
        "    merged_mul1_vgg = Multiply()([x3,x4])\n",
        "    merged_sq1_fn = Add()([x1t,x2t])\n",
        "    merged_sq1_vgg = Add()([x3t,x4t])\n",
        "    merged_sqrt_fn = Lambda(lambda tensor  : signed_sqrt(tensor))(merged_mul1_fn)\n",
        "    merged_sqrt_vgg = Lambda(lambda tensor  : signed_sqrt(tensor))(merged_mul1_vgg)\n",
        "\n",
        "    \n",
        "    merged_add_vgg = Conv2D(128 , [1,1] )(merged_add_vgg)\n",
        "    merged_sub1_vgg = Conv2D(128 , [1,1] )(merged_sub1_vgg)\n",
        "    merged_sub2_vgg = Conv2D(128 , [1,1] )(merged_sub2_vgg)\n",
        "    merged_mul1_vgg = Conv2D(128 , [1,1] )(merged_mul1_vgg)\n",
        "    merged_sq1_vgg = Conv2D(128 , [1,1] )(merged_sq1_vgg)\n",
        "    merged_sqrt_vgg = Conv2D(128 , [1,1] )(merged_sqrt_vgg)\n",
        "    \n",
        "    merged = Concatenate(axis=-1)([Flatten()(merged_add_vgg), (merged_add_fn), Flatten()(merged_sub1_vgg), (merged_sub1_fn),\n",
        "                                   Flatten()(merged_sub2_vgg), (merged_sub2_fn), Flatten()(merged_mul1_vgg), (merged_mul1_fn), \n",
        "                                   Flatten()(merged_sq1_vgg), (merged_sq1_fn), Flatten()(merged_sqrt_vgg), (merged_sqrt_fn)])\n",
        "    \n",
        "    merged = Dense(100, activation=\"relu\")(merged)\n",
        "    merged = Dropout(0.1)(merged)\n",
        "    merged = Dense(25, activation=\"relu\")(merged)\n",
        "    merged = Dropout(0.1)(merged)\n",
        "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
        "\n",
        "    model = Model([input_1, input_2, input_3, input_4], out)\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "    '''\n",
        "    '''\n",
        "def signed_sqrt(x):\n",
        "    return K.sign(x)*K.sqrt(K.abs(x)+1e-9)\n",
        "    '''\n",
        "\n",
        "'''\n",
        "def baseline_model():\n",
        "    #FACENET\n",
        "    facenet_model = load_model('/gdrive/MyDrive/facenet_keras.h5')\n",
        "    for layer in facenet_model.layers[:-3]:\n",
        "        layer.trainable = True\n",
        "    #input\n",
        "    fc_input_1 = Input(shape=(160, 160, 3))        \n",
        "    fc_input_2 = Input(shape=(160, 160, 3))        \n",
        "    #starting model\n",
        "    fn_x1 = facenet_model(fc_input_1)\n",
        "    fn_x2 = facenet_model(fc_input_2)\n",
        "    #reshaping image array for global max pool layer\n",
        "    fn_x1 = Reshape((1, 1 ,128))(fn_x1) \n",
        "    fn_x2 = Reshape((1, 1 ,128))(fn_x2)\n",
        "    #combining inputs\n",
        "    fn_x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn_x1), GlobalAvgPool2D()(fn_x1)])\n",
        "    fn_x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn_x2), GlobalAvgPool2D()(fn_x2)])\n",
        "    #adding potential features, concat to final layer before dense\n",
        "    fn_add = Add()([fn_x1, fn_x2])\n",
        "    fn_product = Multiply()([fn_x1,fn_x2])\n",
        "    fn_x = Concatenate(axis=-1)([fn_add, fn_product])\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB4_fhomW-hl"
      },
      "source": [
        "MODEL ARCHITECTURE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilA8vokynSze",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "def bbaseline_model():\n",
        "    input_1 = Input(shape=(224, 224, 3))\n",
        "    input_2 = Input(shape=(224, 224, 3))\n",
        "\n",
        "    base_model = VGGFace(model='resnet50', include_top=False)\n",
        "\n",
        "    for x in base_model.layers[:-2]:\n",
        "        x.trainable = True\n",
        "\n",
        "    x1 = base_model(input_1)\n",
        "    x2 = base_model(input_2)\n",
        "\n",
        "    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n",
        "    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n",
        "\n",
        "    x3 = Subtract()([x1, x2])\n",
        "    x3 = Multiply()([x3, x3])\n",
        "\n",
        "    x1_ = Multiply()([x1, x1])\n",
        "    x2_ = Multiply()([x2, x2])\n",
        "    x4 = Subtract()([x1_, x2_])\n",
        "    x = Concatenate(axis=-1)([x4, x3])\n",
        "    \n",
        "    x = Dense(512, activation=\"relu\")(x)\n",
        "    x = Dense(256, activation=\"relu\")(x)\n",
        "    x= Dropout(0.01)(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    out = Dense(1, activation=\"sigmoid\")(x)\n",
        "    \n",
        "    model = Model([input_1, input_2], out)\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.0001))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BBZJpieAi7Y"
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization, Add\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "def baseline_model():\n",
        "\n",
        "    ###VGG###\n",
        "    '''\n",
        "    vgg_model = VGGFace(model='vgg16', include_top=False)\n",
        "    for x in vgg_model.layers[:-3]:\n",
        "        x.trainable = True\n",
        "    vgg_input_1 = Input(shape=(224, 224, 3))\n",
        "    vgg_input_2 = Input(shape=(224, 224, 3))\n",
        "    vgg_x1 = BatchNormalization()(vgg_input_1)\n",
        "    vgg_x2 = BatchNormalization()(vgg_input_2)\n",
        "    vgg_x1 = vgg_model(vgg_input_1)\n",
        "    vgg_x2 = vgg_model(vgg_input_2)\n",
        "    #flatten inputs\n",
        "    vgg_x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(vgg_x1), GlobalAvgPool2D()(vgg_x1)])\n",
        "    vgg_x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(vgg_x2), GlobalAvgPool2D()(vgg_x2)])\n",
        "    #adding layers\n",
        "    vgg_x3 = Subtract()([vgg_x1, vgg_x2]) #substract x1 and x2\n",
        "    vgg_x3 = Multiply()([vgg_x3, vgg_x3]) #then square it\n",
        "    vgg_x = Multiply()([vgg_x1, vgg_x2]) #multiply x1 and x2\n",
        "    vgg_x = Concatenate(axis=-1)([vgg_x, vgg_x3]) #concatenate (multiply x1 and x2) with (substract x1 and x2, then square)\n",
        "    '''\n",
        "\n",
        "    ###FACENET###\n",
        "    facenet_model = load_model('/gdrive/MyDrive/facenet_keras.h5')\n",
        "    facenet_model.load_weights('/gdrive/MyDrive/facenet_keras_weights.h5')\n",
        "    for layer in facenet_model.layers[:-3]:\n",
        "        layer.trainable = True\n",
        "    fc_input_1 = Input(shape=(160, 160, 3))\n",
        "    fc_input_2 = Input(shape=(160, 160, 3))\n",
        "    #fc_x1 = BatchNormalization()(fc_input_1)\n",
        "    #fc_x2 = BatchNormalization()(fc_input_2)\n",
        "    fn_x1 = facenet_model(fc_input_1)\n",
        "    fn_x2 = facenet_model(fc_input_2)\n",
        "    #reshaping image array for global max pool layer\n",
        "    fn_x1 = Reshape((1, 1 ,128))(fn_x1) \n",
        "    fn_x2 = Reshape((1, 1 ,128))(fn_x2)\n",
        "    #combining inputs\n",
        "    fn_x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn_x1), GlobalAvgPool2D()(fn_x1)])\n",
        "    fn_x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn_x2), GlobalAvgPool2D()(fn_x2)])\n",
        "    #adding potential features, concat to final layer before dense\n",
        "    fn_add = Add()([fn_x1, fn_x2])\n",
        "    fn_product = Multiply()([fn_x1,fn_x2])\n",
        "    fn_x = Concatenate(axis=-1)([fn_add, fn_product])\n",
        "\n",
        "    '''\n",
        "    ###RESNET###\n",
        "    res_model = VGGFace(model='resnet50', include_top=False)\n",
        "    for x in res_model.layers[:-3]:\n",
        "        x.trainable = True\n",
        "    res_input_1 = Input(shape=(224, 224, 3)) #input tensor shape\n",
        "    res_input_2 = Input(shape=(224, 224, 3))\n",
        "    #res_x1 = BatchNormalization()(res_input_1)\n",
        "    #res_x2 = BatchNormalization()(res_input_2)\n",
        "    res_x1 = res_model(res_input_1) #reshaping input of model to that of image shapes\n",
        "    res_x2 = res_model(res_input_2) #requries two resnet archs\n",
        "    #flatten inputs\n",
        "    res_x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(res_x1), GlobalAvgPool2D()(res_x1)])\n",
        "    res_x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(res_x2), GlobalAvgPool2D()(res_x2)])\n",
        "    #adding potential features, concat to final layer before dense\n",
        "    res_x3 = Subtract()([res_x2, res_x2])\n",
        "    res_x3 = Multiply()([res_x3, res_x3])\n",
        "    res_x1_ = Multiply()([res_x1, res_x1])\n",
        "    res_x2_ = Multiply()([res_x2, res_x2])\n",
        "    res_x4 = Subtract()([res_x1_, res_x2_])\n",
        "    res_x = Concatenate(axis=-1)([res_x4, res_x3])\n",
        "    '''\n",
        "\n",
        "    #MERGE FACENET AND RESNET\n",
        "    '''\n",
        "    merged = Concatenate(axis=-1)([vgg_x, res_x])\n",
        "    merged = Dense(100, activation=\"relu\")(merged)\n",
        "    merged = Dropout(0.01)(merged)\n",
        "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
        "    '''\n",
        "    \n",
        "\n",
        "    #merged = Concatenate(axis=-1)([fn_x, res_x])\n",
        "    merged = Dense(100, activation=\"relu\")(fn_x)\n",
        "    merged = Dropout(0.01)(merged)\n",
        "    out = Dense(1, activation=\"sigmoid\")(merged)\n",
        "\n",
        "    model = Model([fc_input_1, fc_input_2], out)\n",
        "    #model = Model([fc_input_1, fc_input_2, res_input_1, res_input_2], out)\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC3TNCLWx5RC"
      },
      "source": [
        "MODEL AND CHECKPOINTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YEQ0Q6Ui6NP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b927b4-1fb9-4d95-b690-a0c22867af43"
      },
      "source": [
        "'''\n",
        "Save the best model to your drive after each training epoch so that you can come back to it. ReduceLROnPlateau reduces the learning rate when a metric has stopped improving, in this case the validation accuracy. \n",
        "'''\n",
        "file_path = \"/gdrive/MyDrive/vgg_face_tta.h5\"\n",
        "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
        "reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n",
        "callbacks_list = [checkpoint, reduce_on_plateau]\n",
        "model = baseline_model()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "inception_resnet_v1 (Functional (None, 128)          22808144    input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 1, 1, 128)    0           inception_resnet_v1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 1, 1, 128)    0           inception_resnet_v1[1][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_2 (GlobalM (None, 128)          0           reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_2 (Glo (None, 128)          0           reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_3 (GlobalM (None, 128)          0           reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_3 (Glo (None, 128)          0           reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 256)          0           global_max_pooling2d_2[0][0]     \n",
            "                                                                 global_average_pooling2d_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 256)          0           global_max_pooling2d_3[0][0]     \n",
            "                                                                 global_average_pooling2d_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_1 (TFOpLam (None, 256)          0           concatenate_3[0][0]              \n",
            "                                                                 concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 256)          0           concatenate_3[0][0]              \n",
            "                                                                 concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 512)          0           tf.__operators__.add_1[0][0]     \n",
            "                                                                 multiply_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 100)          51300       concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 100)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            101         dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 22,859,545\n",
            "Trainable params: 22,830,713\n",
            "Non-trainable params: 28,832\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpfb9vXxXYFn"
      },
      "source": [
        "FIT MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDQn3ZdZAnX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e58a0a-7e6d-4b2d-afd0-443ad52b1f3a"
      },
      "source": [
        "#model.load_weights('/gdrive/MyDrive/vgg_face_tta.h5')\n",
        "model.fit(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=False,\n",
        "                validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=25, verbose=1,\n",
        "                workers=1, callbacks=callbacks_list, steps_per_epoch=100, validation_steps=50)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550f138dd0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f138dd0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550f138dd0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f138dd0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550f138ef0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f138ef0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550f138ef0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f138ef0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550f1514d0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f1514d0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550f1514d0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f1514d0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550f1517a0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f1517a0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550f1517a0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f1517a0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550f151950> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f151950>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550f151950> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f151950>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550f151b00> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f151b00>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550f151b00> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f151b00>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550f151c20> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f151c20>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550f151c20> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f151c20>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550f151d40> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f151d40>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550f151d40> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f151d40>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550f151e60> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f151e60>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550f151e60> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f151e60>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550f151f80> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f151f80>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550f151f80> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f151f80>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550ee95050> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee95050>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550ee95050> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee95050>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550ee95290> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee95290>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550ee95290> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee95290>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550ee953b0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee953b0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550ee953b0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee953b0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550ee954d0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee954d0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550ee954d0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee954d0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550ee955f0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee955f0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550ee955f0> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee955f0>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550ee95680> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee95680>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550ee95680> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee95680>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550ee95830> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee95830>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550ee95830> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee95830>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550ee95950> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee95950>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550ee95950> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee95950>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550ee95a70> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee95a70>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550ee95a70> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee95a70>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550ee95b90> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee95b90>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550ee95b90> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550ee95b90>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f550f151290> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f151290>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function <lambda> at 0x7f550f151290> and will run it as-is.\n",
            "Cause: could not parse the source code of <function <lambda> at 0x7f550f151290>: no matching AST found\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "100/100 [==============================] - 887s 8s/step - loss: 0.8081 - acc: 0.5937 - val_loss: 1.0564 - val_acc: 0.4463\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.44628, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 2/25\n",
            "100/100 [==============================] - 789s 8s/step - loss: 0.4814 - acc: 0.7772 - val_loss: 0.8700 - val_acc: 0.5302\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.44628 to 0.53020, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 3/25\n",
            "100/100 [==============================] - 806s 8s/step - loss: 0.3794 - acc: 0.8366 - val_loss: 0.7417 - val_acc: 0.6155\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.53020 to 0.61551, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 4/25\n",
            "100/100 [==============================] - 815s 8s/step - loss: 0.2562 - acc: 0.9088 - val_loss: 0.6152 - val_acc: 0.7455\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.61551 to 0.74551, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 5/25\n",
            "100/100 [==============================] - 822s 8s/step - loss: 0.2463 - acc: 0.9224 - val_loss: 0.5040 - val_acc: 0.7829\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.74551 to 0.78291, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 6/25\n",
            "100/100 [==============================] - 818s 8s/step - loss: 0.1699 - acc: 0.9567 - val_loss: 0.4104 - val_acc: 0.8218\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.78291 to 0.82178, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 7/25\n",
            "100/100 [==============================] - 819s 8s/step - loss: 0.1469 - acc: 0.9538 - val_loss: 0.3850 - val_acc: 0.8299\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.82178 to 0.82993, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 8/25\n",
            "100/100 [==============================] - 823s 8s/step - loss: 0.1242 - acc: 0.9762 - val_loss: 0.3625 - val_acc: 0.8485\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.82993 to 0.84854, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 9/25\n",
            "100/100 [==============================] - 807s 8s/step - loss: 0.1105 - acc: 0.9765 - val_loss: 0.2791 - val_acc: 0.8786\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.84854 to 0.87864, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 10/25\n",
            "100/100 [==============================] - 820s 8s/step - loss: 0.1236 - acc: 0.9775 - val_loss: 0.2720 - val_acc: 0.8729\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.87864\n",
            "Epoch 11/25\n",
            "100/100 [==============================] - 802s 8s/step - loss: 0.0837 - acc: 0.9870 - val_loss: 0.2234 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.87864 to 0.91667, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 12/25\n",
            "100/100 [==============================] - 795s 8s/step - loss: 0.0827 - acc: 0.9806 - val_loss: 0.1987 - val_acc: 0.9171\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.91667 to 0.91709, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 13/25\n",
            "100/100 [==============================] - 799s 8s/step - loss: 0.0767 - acc: 0.9838 - val_loss: 0.1965 - val_acc: 0.9119\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.91709\n",
            "Epoch 14/25\n",
            "100/100 [==============================] - 806s 8s/step - loss: 0.0666 - acc: 0.9874 - val_loss: 0.1874 - val_acc: 0.9199\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.91709 to 0.91987, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 15/25\n",
            "100/100 [==============================] - 821s 8s/step - loss: 0.0538 - acc: 0.9872 - val_loss: 0.1590 - val_acc: 0.9303\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.91987 to 0.93027, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 16/25\n",
            "100/100 [==============================] - 794s 8s/step - loss: 0.0591 - acc: 0.9848 - val_loss: 0.1583 - val_acc: 0.9312\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.93027 to 0.93120, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 17/25\n",
            "100/100 [==============================] - 798s 8s/step - loss: 0.0586 - acc: 0.9825 - val_loss: 0.1422 - val_acc: 0.9306\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.93120\n",
            "Epoch 18/25\n",
            "100/100 [==============================] - 807s 8s/step - loss: 0.0613 - acc: 0.9881 - val_loss: 0.1504 - val_acc: 0.9338\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.93120 to 0.93376, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 19/25\n",
            "100/100 [==============================] - 795s 8s/step - loss: 0.0548 - acc: 0.9893 - val_loss: 0.1134 - val_acc: 0.9454\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.93376 to 0.94542, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 20/25\n",
            "100/100 [==============================] - 798s 8s/step - loss: 0.0505 - acc: 0.9913 - val_loss: 0.1281 - val_acc: 0.9460\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.94542 to 0.94599, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 21/25\n",
            "100/100 [==============================] - 806s 8s/step - loss: 0.0431 - acc: 0.9911 - val_loss: 0.1455 - val_acc: 0.9203\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.94599\n",
            "Epoch 22/25\n",
            "100/100 [==============================] - 831s 8s/step - loss: 0.0417 - acc: 0.9942 - val_loss: 0.1794 - val_acc: 0.9194\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.94599\n",
            "Epoch 23/25\n",
            "100/100 [==============================] - 819s 8s/step - loss: 0.0330 - acc: 0.9957 - val_loss: 0.1148 - val_acc: 0.9472\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.94599 to 0.94719, saving model to /gdrive/MyDrive/vgg_face_tta.h5\n",
            "Epoch 24/25\n",
            "100/100 [==============================] - 808s 8s/step - loss: 0.0308 - acc: 0.9976 - val_loss: 0.1729 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.94719\n",
            "Epoch 25/25\n",
            "100/100 [==============================] - 803s 8s/step - loss: 0.0311 - acc: 0.9959 - val_loss: 0.1457 - val_acc: 0.9233\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.94719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f550e9ce3d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZSMtMvfXd6w"
      },
      "source": [
        "PREDICTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIl075HvEfAf"
      },
      "source": [
        "# Modify paths as per your need\n",
        "test_path = \"/gdrive/MyDrive/Kinship Recognition Starter/test/\"\n",
        "\n",
        "model = baseline_model()\n",
        "model.load_weights(\"/gdrive/MyDrive/vgg_face_tta.h5\")\n",
        "\n",
        "submission = pd.read_csv('/gdrive/MyDrive/Kinship Recognition Starter/test_ds.csv')\n",
        "predictions = []\n",
        "\n",
        "for i in range(0, len(submission.p1.values), 32):\n",
        "    if i%64 == 0:\n",
        "      print(i)\n",
        "    X1 = submission.p1.values[i:i+32]\n",
        "    X1 = np.array([read_img_fc(test_path + x) for x in X1])\n",
        "\n",
        "    X2 = submission.p2.values[i:i+32]\n",
        "    X2 = np.array([read_img_fc(test_path + x) for x in X2])\n",
        "\n",
        "    pred = model.predict([X1, X2]).ravel().tolist()\n",
        "    predictions += pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXyzufq7iZ1g"
      },
      "source": [
        "CREATE CSV TO SUBMIT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bObDsMSLN_12"
      },
      "source": [
        "d = {'index': np.arange(0, 3000, 1), 'label':predictions}\n",
        "submissionfile = pd.DataFrame(data=d)\n",
        "count = 0\n",
        "count1 = 0\n",
        "count0 = 0\n",
        "for i in submissionfile.iloc[:, 1]:\n",
        "  if i < 0.8:\n",
        "    submissionfile.at[count, 'label'] = 0\n",
        "    count0 +=1\n",
        "  else:\n",
        "    submissionfile.at[count, 'label'] = 1\n",
        "    count1 +=1\n",
        "  count+=1\n",
        "  if count % 100 == 0:\n",
        "    print(count)\n",
        "print(\"1 count:\", count1)\n",
        "print(\"0 count:\", count0)\n",
        "#submissionfile = submissionfile.round()\n",
        "\n",
        "submissionfile.astype(\"int64\").to_csv(\"/gdrive/MyDrive/facenet7.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9F1Nr97eA0G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvnJu7sgaiVL",
        "outputId": "db9ccd5d-6100-4475-c359-8b733d30758b"
      },
      "source": [
        "print(submissionfile.iloc[2999, 1])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzPgiIEiXlS-",
        "outputId": "720de651-0a0f-4b03-c5ae-3632a8892377"
      },
      "source": [
        "!ls /gdrive/MyDrive/\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Colab Notebooks'\t\t facenet6.csv\n",
            "'dense_resnet(92_val_acc).csv'\t facenet_keras.h5\n",
            "'dense_resnet(93_val_acc).csv'\t facenet_keras_weights.h5\n",
            " facenet2.csv\t\t\t'Getting started.pdf'\n",
            " facenet3.csv\t\t\t'Kinship Recognition Starter'\n",
            " facenet4.csv\t\t\t vgg_face_tta.h5\n",
            " facenet5.csv\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}